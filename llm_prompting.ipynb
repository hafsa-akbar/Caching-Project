{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from openai import OpenAI\n",
    "import base64\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Across Articles' Image comparison with GPT-4o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we first supply the 'training' similarity scores in the form of a matrix for the images given in the train folder<br>The matrix for n images is an n x n *symmetric* matrix with (nC2) comparisons\n",
    "e.g,\n",
    "|       | img0  | img1  | \n",
    "|-------|-------|-------|\n",
    "| **img0** | 1.0   | x  | \n",
    "| **img1** | x  | 1.0   | \n",
    "\n",
    "\n",
    "This similarity matrix serves as the few shot example training for the LLM<br>Feel free to change/add to the training images in the train folder and redefine the training similarity matrix\n",
    "\n",
    "**Note: api calls charged per usage ~0.5 usd for every 5 image comparisons (5x5 simlarity matrix - 10 comparisons) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "openai_api_key = os.getenv('api_key')\n",
    "api_key = openai_api_key\n",
    "gpt_client = OpenAI(api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.auth.transport.requests import Request\n",
    "from google.oauth2.service_account import Credentials\n",
    "from anthropic import AnthropicVertex\n",
    "\n",
    "key_path = 'caching-436119-3f7e7f2329ed.json'\n",
    "\n",
    "credentials = Credentials.from_service_account_file(\n",
    "    key_path,\n",
    "    scopes=['https://www.googleapis.com/auth/cloud-platform'])\n",
    "\n",
    "if credentials.expired:\n",
    "    credentials.refresh(Request())\n",
    "\n",
    "PROJECT_ID = 'caching-436119'\n",
    "REGION = 'us-east5'\n",
    "\n",
    "claude_client = AnthropicVertex(project_id=PROJECT_ID, region=REGION, credentials=credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_number(filename, article_num=False):\n",
    "    match = re.search(r'image_(\\d+)_(\\d+)', filename) \n",
    "    group_num = 1 if article_num else 2\n",
    "    return int(match.group(group_num))\n",
    "\n",
    "def load_image_as_base64(image_path):\n",
    "    with open(image_path, 'rb') as img_file:\n",
    "        return base64.b64encode(img_file.read()).decode('utf-8')\n",
    "\n",
    "def read_matrix_from_csv(filepath):\n",
    "    with open(filepath, 'r') as csv_file:\n",
    "        reader = csv.reader(csv_file)\n",
    "        matrix = [list(map(float, row)) for row in reader]\n",
    "    return np.array(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_category(category, train_dir='train', model_type='gpt'):\n",
    "    categories = [d for d in os.listdir(train_dir) if os.path.isdir(os.path.join(train_dir, d))]\n",
    "    prompt = f\"Which of these categories: {', '.join(categories)} is most similar to '{category}'? You CANNOT choose a category not mentioned in the given list \\\n",
    "    Respond ONLY with the category name without any additional words or punctuation.\"\n",
    "\n",
    "    try:\n",
    "        if model_type == 'gpt':\n",
    "            response = gpt_client.chat.completions.create(\n",
    "                model=\"gpt-4o\",  \n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                max_tokens=10\n",
    "            )\n",
    "            similar_category = response.choices[0].message['content'].strip()\n",
    "        \n",
    "        elif model_type == 'claude':\n",
    "            response = claude_client.messages.create(\n",
    "                model=\"claude-3-5-sonnet@20240620\",\n",
    "                max_tokens=10,\n",
    "                stream = False,\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": prompt,\n",
    "                    }\n",
    "                ]\n",
    "            )\n",
    "            similar_category = response.content[0].text\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in finding similar category: {e}\")\n",
    "        return None\n",
    "    \n",
    "    return similar_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def few_shot(category, static_folder=None, model_type='gpt'):\n",
    "    if static_folder is None:\n",
    "        similar_category = find_similar_category(category, model_type=model_type)\n",
    "        print(f'category similar to {category}: {similar_category}')\n",
    "        images_path = os.path.join('train', similar_category)\n",
    "    else:\n",
    "        images_path = static_folder\n",
    "\n",
    "    labels_csv = os.path.join(images_path, 'labels.csv')\n",
    "    similarity_csv = os.path.join(images_path, 'similarity.csv')\n",
    "\n",
    "    image_paths = sorted(\n",
    "        [os.path.join(images_path, f) for f in os.listdir(images_path) if f.endswith(('jpg', 'jpeg'))],\n",
    "        key=extract_number\n",
    "    )\n",
    "    \n",
    "    labels = pd.read_csv(labels_csv)\n",
    "    similarity_scores = read_matrix_from_csv(similarity_csv)\n",
    "    few_shot_examples = create_examples(image_paths, labels, similarity_scores, model_type)\n",
    "    \n",
    "    return few_shot_examples\n",
    "\n",
    "def create_examples(image_paths, labels, similarity_scores, model_type='gpt'):\n",
    "    few_shot_examples = []\n",
    "    for i in range(len(image_paths)):\n",
    "        for j in range(i + 1, len(image_paths)):\n",
    "            base64_img1 = load_image_as_base64(image_paths[i])\n",
    "            base64_img2 = load_image_as_base64(image_paths[j])\n",
    "            \n",
    "            img1_number = extract_number(image_paths[i])\n",
    "            img2_number = extract_number(image_paths[j])\n",
    "            \n",
    "            alt_text1 = labels.loc[labels['image number'] == img1_number, 'alt'].values[0]\n",
    "            heading1 = labels.loc[labels['image number'] == img1_number, 'article_heading'].values[0]\n",
    "            alt_text2 = labels.loc[labels['image number'] == img2_number, 'alt'].values[0]\n",
    "            heading2 = labels.loc[labels['image number'] == img2_number, 'article_heading'].values[0]\n",
    "\n",
    "            image1_content = {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_img1}\"}\n",
    "            }\n",
    "            image2_content = {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_img2}\"}\n",
    "            }\n",
    "            \n",
    "            if model_type == 'claude':\n",
    "                image1_content = {\n",
    "                    \"type\": \"image\",\n",
    "                    \"source\": {\n",
    "                        \"type\": \"base64\",\n",
    "                        \"media_type\": \"image/jpeg\",\n",
    "                        \"data\": base64_img1\n",
    "                    }\n",
    "                }\n",
    "                image2_content = {\n",
    "                    \"type\": \"image\",\n",
    "                    \"source\": {\n",
    "                        \"type\": \"base64\",\n",
    "                        \"media_type\": \"image/jpeg\",\n",
    "                        \"data\": base64_img2\n",
    "                    }\n",
    "                }\n",
    "\n",
    "            few_shot_examples.append({\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": f\"Image 1 Alt Text: {alt_text1}\\nImage 1 Heading: {heading1}\"\n",
    "                },\n",
    "                image1_content,\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": f\"Image 2 Alt Text: {alt_text2}\\nImage 2 Heading: {heading2}\"\n",
    "                },\n",
    "                image2_content,\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": f\"Question: Compute semantic similarity score (on 0-4 scale) for the above pair of images,\\\n",
    "                        considering the content, context, alt text, and article headings i.e, how replaceable is one image \\\n",
    "                        with the another (0-4 scale). \\nAnswer: {similarity_scores[i][j]}\"\n",
    "                }]\n",
    "            })\n",
    "    return few_shot_examples\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_images(image_paths, labels, few_shot_examples, model_type='gpt'):\n",
    "    user_messages = []\n",
    "\n",
    "    for i in range(len(image_paths)):\n",
    "        for j in range(i + 1, len(image_paths)):\n",
    "            article_num1 = extract_number(image_paths[i], article_num=True)\n",
    "            article_num2 = extract_number(image_paths[j], article_num=True)\n",
    "\n",
    "            if article_num1 == article_num2:\n",
    "                user_messages.append('')\n",
    "                continue\n",
    "\n",
    "            base64_img1 = load_image_as_base64(image_paths[i])\n",
    "            base64_img2 = load_image_as_base64(image_paths[j])\n",
    "\n",
    "            if model_type == 'gpt':\n",
    "                image_content1 = {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_img1}\"}}\n",
    "                image_content2 =  {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_img2}\"}}\n",
    "            elif model_type == 'claude':\n",
    "                image_content1 = {\n",
    "                    \"type\": \"image\",\n",
    "                    \"source\": {\n",
    "                        \"type\": \"base64\",\n",
    "                        \"media_type\": \"image/jpeg\",\n",
    "                        \"data\": base64_img1  \n",
    "                    }\n",
    "                }\n",
    "                image_content2 = {\n",
    "                    \"type\": \"image\",\n",
    "                    \"source\": {\n",
    "                        \"type\": \"base64\",\n",
    "                        \"media_type\": \"image/jpeg\",\n",
    "                        \"data\": base64_img2 \n",
    "                    }\n",
    "                }\n",
    "            \n",
    "            img1_number = extract_number(image_paths[i]) \n",
    "            img2_number = extract_number(image_paths[j])\n",
    "            alt_text1 = labels.loc[(labels['article_number'] == article_num1) & (labels['image number'] == img1_number), 'alt'].values[0]\n",
    "            heading1 = labels.loc[(labels['article_number'] == article_num1) & (labels['image number'] == img1_number), 'article_heading'].values[0]\n",
    "\n",
    "            alt_text2 = labels.loc[(labels['article_number'] == article_num2) & (labels['image number'] == img2_number), 'alt'].values[0]\n",
    "            heading2 = labels.loc[(labels['article_number'] == article_num2) & (labels['image number'] == img2_number), 'article_heading'].values[0]\n",
    "\n",
    "            user_messages.append({\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": f\"Image 1 Alt Text: {alt_text1}\\nImage 1 Heading: {heading1}\"},\n",
    "                    image_content1,\n",
    "\n",
    "                    {\"type\": \"text\", \"text\": f\"Image 2 Alt Text: {alt_text2}\\nImage 2 Heading: {heading2}\"},\n",
    "                    image_content2, \n",
    "\n",
    "                    {\"type\": \"text\",\n",
    "                    \"text\": \"Question: On a scale from 0 to 4 (0: Not replaceable, 1: Slightly replaceable, 2: Moderately replaceable, \\\n",
    "                        3: Very replaceable, 4: Completely replaceable), rate the similarity of these two images based on the images themselves, \\\n",
    "                        their alt text descriptions, and the article headings where they are used. Don't be too generous with the ratings. Please respond with ONLY the numerical score, \\\n",
    "                        without any additional text or punctuation. \"}\n",
    "                ]\n",
    "            })\n",
    "\n",
    "    responses = []\n",
    "    for message in tqdm(user_messages, desc=\"Processing image pairs\"):\n",
    "        if not message:\n",
    "            responses.append(0)\n",
    "            continue\n",
    "        \n",
    "        try:    \n",
    "            if model_type == 'gpt':\n",
    "                response = gpt_client.chat.completions.create(\n",
    "                    model=\"gpt-4o\", \n",
    "                    messages=few_shot_examples + [message],\n",
    "                    max_tokens=10\n",
    "                )\n",
    "                res = response.choices[0].message['content'].strip()\n",
    "\n",
    "            elif model_type == 'claude':\n",
    "                response = claude_client.messages.create(\n",
    "                    model=\"claude-3-5-sonnet@20240620\",\n",
    "                    max_tokens=10,\n",
    "                    messages=[\n",
    "                        {\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": message['content'],\n",
    "                        }\n",
    "                    ],\n",
    "                    stream=False\n",
    "                )\n",
    "                res = response.content[0].text\n",
    "\n",
    "            score = re.findall(r'-?\\d*\\.?\\d+', res)[0]\n",
    "            responses.append(int(score))\n",
    "            time.sleep(15)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            responses.append(0)\n",
    "        \n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filename(image_path):\n",
    "    return os.path.splitext(os.path.basename(image_path))[0]\n",
    "\n",
    "def make_matrix(image_paths, similarity_scores):\n",
    "    image_names = [get_filename(path) for path in image_paths]\n",
    "    \n",
    "    n = len(image_paths)\n",
    "    similarity_matrix = [[4 if i == j else 0 for j in range(n)] for i in range(n)]\n",
    "    \n",
    "    idx = 0\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            similarity_matrix[i][j] = int(similarity_scores[idx])\n",
    "            similarity_matrix[j][i] = int(similarity_scores[idx])\n",
    "            idx += 1\n",
    "    \n",
    "    df = pd.DataFrame(similarity_matrix, index=image_names, columns=image_names)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def process_categories(base_dir, static_folder=None, zero_shot=False, model_type='gpt'):\n",
    "    def get_dirs(base_dir):\n",
    "        return [d for d in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, d))]\n",
    "\n",
    "    website_folders = get_dirs(base_dir)\n",
    "    \n",
    "\n",
    "    for website in website_folders:\n",
    "        website_dir = os.path.join(base_dir, website)\n",
    "        category_folders = get_dirs(website_dir)\n",
    "\n",
    "        for category in category_folders:\n",
    "            category_dir = os.path.join(website_dir, category)\n",
    "            labels = pd.read_csv(os.path.join(category_dir, 'image_data.csv'))\n",
    "\n",
    "            image_paths = sorted(\n",
    "                [os.path.join(category_dir, f) for f in os.listdir(category_dir) if f.endswith(('jpg', 'jpeg'))],\n",
    "                key=lambda x: (extract_number(x, article_num=True), extract_number(x))\n",
    "            )\n",
    "            if len(image_paths) < 2:\n",
    "                continue\n",
    "\n",
    "            few_shot_examples = []\n",
    "            if not zero_shot:\n",
    "                few_shot_examples = few_shot(category, static_folder, model_type=model_type)\n",
    "\n",
    "            similarity_scores = compare_images(image_paths, labels, few_shot_examples, model_type=model_type)\n",
    "            similarity_df = make_matrix(image_paths, similarity_scores)\n",
    "\n",
    "            few_shot_out = 'with_fewshot' if not zero_shot else f'without_fewshot'\n",
    "            done_testing_dir = os.path.join(base_dir, '..', f'done_testing/{few_shot_out}', website)\n",
    "            os.makedirs(done_testing_dir, exist_ok=True)\n",
    "\n",
    "            shutil.move(category_dir, os.path.join(done_testing_dir, category))\n",
    "            similarity_df.to_csv(f'pred_labels/{few_shot_out}/{model_type}/{website}_{category}.csv')\n",
    "            \n",
    "        shutil.rmtree(website_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper functions to compute RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rmse_with_filter(pred_df, true_df, tolerance=0):\n",
    "    valid_pairs_pred = []\n",
    "    valid_pairs_true = []\n",
    "    \n",
    "    columns = pred_df.columns\n",
    "    for i in range(len(columns)):\n",
    "        for j in range(i + 1, len(columns)):\n",
    "            img_i = columns[i].split('_')[1]\n",
    "            img_j = columns[j].split('_')[1]\n",
    "            \n",
    "            if img_i != img_j:\n",
    "                valid_pairs_pred.append(pred_df.iloc[i, j])\n",
    "                valid_pairs_true.append(true_df.iloc[i, j])\n",
    "\n",
    "    if valid_pairs_pred and valid_pairs_true:\n",
    "        squared_errors = []\n",
    "        for pred_val, true_val in zip(valid_pairs_pred, valid_pairs_true):\n",
    "            difference = abs(pred_val - true_val)\n",
    "            error = max(0, difference - tolerance)\n",
    "            squared_errors.append(error ** 2)\n",
    "\n",
    "        if squared_errors:\n",
    "            mse = np.mean(squared_errors)\n",
    "            rmse = np.sqrt(mse)\n",
    "            return rmse/4.0\n",
    "    return None\n",
    "\n",
    "def compute_average_rmse(pred_dir, true_dir, tolerance=0):\n",
    "    rmse_scores = []\n",
    "\n",
    "    pred_files = [f for f in os.listdir(pred_dir) if f.endswith('.csv')]\n",
    "\n",
    "    for pred_file in pred_files:\n",
    "        parts = pred_file.split('_')\n",
    "        website_name = parts[0]\n",
    "        category = \"_\".join(parts[1:]).replace('.csv', '')\n",
    "\n",
    "        true_file = os.path.join(true_dir, website_name, f'{website_name} - {category}.csv')\n",
    "\n",
    "        if os.path.exists(true_file):\n",
    "            pred_df = pd.read_csv(os.path.join(pred_dir, pred_file), index_col=0)\n",
    "            true_df = pd.read_csv(true_file, index_col=0)\n",
    "\n",
    "            if pred_df.shape == true_df.shape:\n",
    "                rmse = compute_rmse_with_filter(pred_df, true_df, tolerance=tolerance)\n",
    "                if rmse is not None:\n",
    "                    rmse_scores.append(rmse)\n",
    "                    print(f\"RMSE for {website_name} - {category}: {rmse}\")\n",
    "                else:\n",
    "                    print(f\"No valid pairs found for {website_name} - {category}\")\n",
    "            else:\n",
    "                print(f\"Shape mismatch for {website_name} - {category}\")\n",
    "        else:\n",
    "            print(f\"True file not found for {website_name} - {category}\")\n",
    "\n",
    "    if rmse_scores:\n",
    "        avg_rmse = sum(rmse_scores) / len(rmse_scores)\n",
    "        print(f\"Average RMSE: {avg_rmse}\")\n",
    "        return avg_rmse\n",
    "    else:\n",
    "        print(\"No RMSE scores were computed.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute Weighted Cohen's Kappa as error measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "def compute_kappa_with_filter(pred_df, true_df, weights):\n",
    "    valid_pairs_pred = []\n",
    "    valid_pairs_true = []\n",
    "    \n",
    "    columns = pred_df.columns\n",
    "    for i in range(len(columns)):\n",
    "        for j in range(i + 1, len(columns)):  \n",
    "            img_i = columns[i].split('_')[1]\n",
    "            img_j = columns[j].split('_')[1]\n",
    "            \n",
    "            if img_i != img_j: \n",
    "                valid_pairs_pred.append(pred_df.iloc[i, j])\n",
    "                valid_pairs_true.append(true_df.iloc[i, j])\n",
    "\n",
    "    if valid_pairs_pred and valid_pairs_true:\n",
    "        return cohen_kappa_score(valid_pairs_true, valid_pairs_pred, weights=weights)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def compute_average_kappa(pred_dir, true_dir, weights=\"linear\"):\n",
    "    kappa_scores = []\n",
    "\n",
    "    pred_files = [f for f in os.listdir(pred_dir) if f.endswith('.csv')]\n",
    "\n",
    "    for pred_file in pred_files:\n",
    "        parts = pred_file.split('_')\n",
    "        website_name = parts[0]\n",
    "        category = \"_\".join(parts[1:]).replace('.csv', '')\n",
    "\n",
    "        true_file = os.path.join(true_dir, website_name, f'{website_name} - {category}.csv')\n",
    "        \n",
    "        if os.path.exists(true_file):\n",
    "            pred_df = pd.read_csv(os.path.join(pred_dir, pred_file), index_col=0)\n",
    "            true_df = pd.read_csv(true_file, index_col=0)\n",
    "            \n",
    "            if pred_df.shape == true_df.shape:\n",
    "                kappa = compute_kappa_with_filter(pred_df, true_df, weights=weights)\n",
    "                if kappa is not None:\n",
    "                    kappa_scores.append(kappa)\n",
    "                    print(f\"Cohen's Kappa for {website_name} - {category}: {kappa}\")\n",
    "            else:\n",
    "                print(f\"Shape mismatch for {website_name} - {category}\")\n",
    "        else:\n",
    "            print(f\"True file not found for {website_name} - {category}\")\n",
    "\n",
    "    if kappa_scores:\n",
    "        avg_kappa = sum(kappa_scores) / len(kappa_scores)\n",
    "        print(f\"Average Cohen's Kappa: {avg_kappa}\")\n",
    "        return avg_kappa\n",
    "    else:\n",
    "        print(\"No Kappa scores were computed.\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute Average of True Labels between two raters and ICC score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_matrix_to_csv(output_file, avg_matrix):\n",
    "    with open(output_file, 'w', newline='') as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        writer.writerows(avg_matrix.astype(int))\n",
    "\n",
    "def compute_average_matrix(matrix1, matrix2):\n",
    "    return np.round((matrix1 + matrix2) / 2, 0)\n",
    "\n",
    "def extract_base_filename(filename):\n",
    "    base_name, _ = os.path.splitext(filename) \n",
    "    return re.sub(r'\\d+$', '', base_name)\n",
    "\n",
    "def process_labels(base_dir):\n",
    "    for category in os.listdir(base_dir):\n",
    "        category_path = os.path.join(base_dir, category)\n",
    "        \n",
    "        if not os.path.isdir(category_path):\n",
    "            continue\n",
    "        \n",
    "        files = sorted([f for f in os.listdir(category_path) if f.endswith('.csv')])\n",
    "\n",
    "        for i in range(0, len(files), 2):\n",
    "            file1 = files[i]\n",
    "            file2 = files[i+1] if i+1 < len(files) else None\n",
    "            \n",
    "            if file2 is None:\n",
    "                continue\n",
    "            \n",
    "            matrix1 = read_matrix_from_csv(os.path.join(category_path, file1))\n",
    "            matrix2 = read_matrix_from_csv(os.path.join(category_path, file2))\n",
    "            \n",
    "            avg_matrix = compute_average_matrix(matrix1, matrix2)\n",
    "\n",
    "            output_file = f'true_labels/{category}_{extract_base_filename(file1)}.csv'\n",
    "            os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "            \n",
    "            write_matrix_to_csv(output_file, avg_matrix)\n",
    "    \n",
    "    print('Average similarity matrices(labels) written to files')\n",
    "\n",
    "#process_labels('true_labels/old')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pingouin as pg\n",
    "\n",
    "def flatten_upper_triangle(matrix):\n",
    "    return matrix[np.triu_indices_from(matrix, 1)]\n",
    "\n",
    "def icc(flat_matrices_by_rater):\n",
    "    df = pd.DataFrame(flat_matrices_by_rater)\n",
    "    \n",
    "    df['Target'] = df.index \n",
    "    df_long = pd.melt(df, id_vars='Target', var_name='Rater', value_name='Similarity')\n",
    "    \n",
    "    icc_results = pg.intraclass_corr(data=df_long, targets='Target', raters='Rater', ratings='Similarity')\n",
    "    icc_value = icc_results.iloc[0]['ICC']\n",
    "    \n",
    "    return round(icc_value, 2)\n",
    "\n",
    "def find_icc_per_website(base_dir):\n",
    "    for category in os.listdir(base_dir):\n",
    "        category_path = os.path.join(base_dir, category)\n",
    "        \n",
    "        if not os.path.isdir(category_path):\n",
    "            continue\n",
    "        \n",
    "        files = sorted([f for f in os.listdir(category_path) if f.endswith('.csv')])\n",
    "\n",
    "        matrices_by_rater = {}\n",
    "        \n",
    "        for file in files:\n",
    "            filename, _ = os.path.splitext(file)\n",
    "            rater = filename[-1]\n",
    "            \n",
    "            if rater not in matrices_by_rater:\n",
    "                matrices_by_rater[rater] = []\n",
    "            \n",
    "            matrix = read_matrix_from_csv(os.path.join(category_path, file))\n",
    "            matrices_by_rater[rater].append(matrix)\n",
    "        \n",
    "        flat_matrices_by_rater = {}\n",
    "        \n",
    "        for rater, matrices in matrices_by_rater.items():\n",
    "            flat_matrices = [flatten_upper_triangle(matrix) for matrix in matrices]\n",
    "            flat_data = np.concatenate(flat_matrices)\n",
    "            flat_matrices_by_rater[f'Rater_{rater}'] = flat_data\n",
    "        \n",
    "        if len(flat_matrices_by_rater) > 1:\n",
    "            inter_rater_compatibility = icc(flat_matrices_by_rater)\n",
    "            print(f'Website: {category}')\n",
    "            print(f'Inter-rater reliability (ICC): {inter_rater_compatibility}')\n",
    "        else:\n",
    "            print(f'Not enough matrices to compute ICC for category: {category}')\n",
    "\n",
    "find_icc_per_website('true_labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing image pairs: 100%|██████████| 78/78 [23:49<00:00, 18.33s/it]\n"
     ]
    }
   ],
   "source": [
    "test_dir = 'test'\n",
    "process_categories(test_dir, static_folder=None, model_type='claude') # dynamic few shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cohen's Kappa for NDTV - Education: 0.4130434782608695\n",
      "Cohen's Kappa for NDTV - People: 0.37823834196891193\n",
      "Cohen's Kappa for NDTV - Auto: 0.31411530815109345\n",
      "Cohen's Kappa for NDTV - India: 0.5029239766081872\n",
      "Cohen's Kappa for apnnews.com - NFL: 0.09021856303189346\n",
      "Cohen's Kappa for edition.cnn.com - climate_solutions: 0.04453591009212665\n",
      "Cohen's Kappa for apnnews.com - Music: 0.0\n",
      "Cohen's Kappa for NDTV - Science: 0.784\n",
      "Cohen's Kappa for nbcnews.com - culture: 0.0\n",
      "Cohen's Kappa for apnnews.com - Fact Check: 0.1629569012547737\n",
      "Cohen's Kappa for apnnews.com - US Supreme Court: 0.77947932618683\n",
      "Cohen's Kappa for edition.cnn.com - sleep: 0.4419667336867117\n",
      "Cohen's Kappa for nbcnews.com - business: 0.530842745438749\n",
      "Cohen's Kappa for nbcnews.com - 2024_elections: 0.30219435736677125\n",
      "Cohen's Kappa for edition.cnn.com - politics_congress: 0.5435984687367077\n",
      "Cohen's Kappa for edition.cnn.com - world_middleeast_israel: 0.3586834957660969\n",
      "Cohen's Kappa for apnnews.com - Health: 0.5663304534038034\n",
      "Cohen's Kappa for edition.cnn.com - markets_nightcap: 0.0\n",
      "Cohen's Kappa for nbcnews.com - science_space: 0.5082248332804065\n",
      "Average Cohen's Kappa: 0.3537554154333648\n"
     ]
    }
   ],
   "source": [
    "pred_dir = 'pred_labels/with_fewshot/claude/'\n",
    "true_dir = 'true_labels/new/'\n",
    "\n",
    "average_kappa = compute_average_kappa(pred_dir, true_dir, weights=\"quadratic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for NDTV - Education: 0.18257418583505536\n",
      "RMSE for NDTV - People: 0.14907119849998599\n",
      "RMSE for NDTV - Auto: 0.17873008824606013\n",
      "RMSE for NDTV - India: 0.18633899812498247\n",
      "RMSE for apnnews.com - NFL: 0.3529241795756714\n",
      "RMSE for edition.cnn.com - climate_solutions: 0.2023192986137242\n",
      "RMSE for apnnews.com - Music: 0.09128709291752768\n",
      "RMSE for NDTV - Science: 0.12909944487358055\n",
      "RMSE for nbcnews.com - culture: 0.09415544714433868\n",
      "RMSE for apnnews.com - Fact Check: 0.3312342935223347\n",
      "RMSE for apnnews.com - US Supreme Court: 0.1767766952966369\n",
      "RMSE for edition.cnn.com - sleep: 0.13757200419426974\n",
      "RMSE for nbcnews.com - business: 0.15214515486254615\n",
      "RMSE for nbcnews.com - 2024_elections: 0.27094777801764386\n",
      "RMSE for edition.cnn.com - politics_congress: 0.22132891019134054\n",
      "RMSE for edition.cnn.com - world_middleeast_israel: 0.1739800514310883\n",
      "RMSE for apnnews.com - Health: 0.14223909013659572\n",
      "RMSE for edition.cnn.com - markets_nightcap: 0.11180339887498948\n",
      "RMSE for nbcnews.com - science_space: 0.2528572356275284\n",
      "Average RMSE: 0.1861781339992579\n"
     ]
    }
   ],
   "source": [
    "rmse = compute_average_rmse(pred_dir, true_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Add of Images IN-ARTICLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_links(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    article_links = []\n",
    "\n",
    "    for a_tag in soup.find_all('a', href=True):\n",
    "        if 'article' in a_tag['href']:\n",
    "            heading = a_tag.get('aria-label', None) or a_tag.text.strip()\n",
    "            if heading:  \n",
    "                article_links.append({\n",
    "                    'url': a_tag['href'],\n",
    "                    'heading': heading\n",
    "                })\n",
    "\n",
    "    for heading_tag in soup.find_all(['h1', 'h2', 'h3']):\n",
    "        try:\n",
    "            a_tag = heading_tag.find('a', href=True)\n",
    "            if a_tag in a_tag['href']:\n",
    "                heading = heading_tag.text.strip()\n",
    "                if heading: \n",
    "                    if not any(link['url'] == a_tag['href'] for link in article_links):\n",
    "                        article_links.append({\n",
    "                            'url': a_tag['href'],\n",
    "                            'heading': heading\n",
    "                        })\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return article_links\n",
    "\n",
    "def scrape_article(url, heading):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    article_heading = heading\n",
    "\n",
    "    images = []\n",
    "    for img in soup.find_all('img'):\n",
    "        if img.has_attr('width') and img.has_attr('height'):\n",
    "            width = float(img['width'])\n",
    "            height = float(img['height'])\n",
    "            if width > 150 and height > 150:\n",
    "                images.append({\n",
    "                    'image_url': img['src'],\n",
    "                    'image_alt': img.get('alt', 'No Alt')\n",
    "                })\n",
    "    \n",
    "    main_content = soup.find(id='main-content') or soup.find(id='maincontent') or soup.find(class_='main-content') or soup.find(class_='maincontent')\n",
    "    \n",
    "    content = ''\n",
    "    if main_content:\n",
    "        paragraphs = main_content.find_all('p')\n",
    "        content = ' '.join([p.text for p in paragraphs])\n",
    "\n",
    "    return {\n",
    "        'article_url': url,\n",
    "        'heading': article_heading,\n",
    "        'images': images,\n",
    "        'content': content\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://www.theguardian.com/uk'\n",
    "article_links = get_article_links(base_url)\n",
    "\n",
    "scraped_data = []\n",
    "for article in article_links:\n",
    "    full_url = article['url'] if article['url'].startswith('http') else urljoin(base_url,article[\"url\"])\n",
    "    article_data = scrape_article(full_url, article['heading'])\n",
    "    scraped_data.append(article_data)\n",
    "\n",
    "df = pd.DataFrame(scraped_data)\n",
    "df = df.drop_duplicates(subset='article_url').reset_index(drop=True)\n",
    "\n",
    "base_url_name = urlparse(base_url).netloc.split('.')[1]\n",
    "df.to_csv(f\"article_csvs/{base_url_name}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['images'].apply(lambda x: len(x) > 1)].reset_index(drop=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "def convert_to_list(text):\n",
    "    return ast.literal_eval(text)\n",
    "\n",
    "true_labels = pd.read_csv('article_csvs/true_labels.csv', converters={'true_labels': convert_to_list})\n",
    "\n",
    "true_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "openai_api_key = os.getenv('api_key')\n",
    "client = OpenAI(api_key=openai_api_key)\n",
    "\n",
    "few_shot_example = merged_df.iloc[5]\n",
    "test_df = merged_df.drop(index=5).reset_index(drop=True)\n",
    "\n",
    "few_shot_prompt = (\n",
    "    f\"Imagine you are an editor evaluating the contribution of images to an article. \"\n",
    "    f\"Your task is to rate each image based on how much it enhances the reader's understanding and engagement with the article. \"\n",
    "    f\"Consider the following criteria for rating each image:\\n\"\n",
    "    f\"- **0: Not Important**: The image does not add any value to the article, is redundant, or is irrelevant.\\n\"\n",
    "    f\"- **1: Slightly Important**: The image adds minimal value, such as illustrating a minor point or providing a generic visual.\\n\"\n",
    "    f\"- **2: Somewhat Important**: The image supports the content but is not crucial for understanding the article.\\n\"\n",
    "    f\"- **3: Moderately Important**: The image is helpful in conveying key ideas or themes, making the article more engaging.\\n\"\n",
    "    f\"- **4: Very Important**: The image is essential, significantly enhancing the article by illustrating a central point, setting context, or evoking strong emotions.\\n\\n\"\n",
    "    f\"Please consider the following factors when rating each image:\\n\"\n",
    "    f\"- **Relevance**: How directly does the image relate to the article's main content?\\n\"\n",
    "    f\"- **Visual Impact**: How much does the image contribute to the reader's emotional engagement or understanding of the article?\\n\"\n",
    "    f\"- **Context**: Is the image contextually appropriate, and does it fit well within the article's narrative?\\n\"\n",
    "    f\"- **Redundancy**: Avoid giving high ratings to images that repeat information or perspectives already provided by previous images.\\n\\n\"\n",
    "    f\"Here is an example to guide your ratings:\\n\\n\"\n",
    "    f\"**Article Heading**: {few_shot_example['heading']}\\n\"\n",
    "    f\"**Article URL**: {few_shot_example['article_url']}\\n\"\n",
    "    f\"**Content**: {few_shot_example['content']}\\n\\n\"\n",
    "    \"### Images and their Alt Texts:\\n\"\n",
    ")\n",
    "\n",
    "for idx, image_info in enumerate(few_shot_example['images']):\n",
    "    few_shot_prompt += (\n",
    "        f\"Image {idx + 1} URL: {image_info['image_url']}\\n\"\n",
    "        f\"Image {idx + 1} Alt Text: {image_info['image_alt']}\\n\"\n",
    "    )\n",
    "\n",
    "few_shot_prompt += f\"\\nTrue Labels: {few_shot_example['true_labels']}\\n\"\n",
    "\n",
    "predicted_labels = []\n",
    "\n",
    "for index, row in test_df.iterrows():\n",
    "    prompt = (\n",
    "        f\"{few_shot_prompt}\\n\\n\"\n",
    "        f\"Now, please rate the images for the following article in the same way (ONLY respond with the ratings - don't add any words):\\n\"\n",
    "        f\"Article Heading: {row['heading']}\\n\"\n",
    "        f\"Article URL: {row['article_url']}\\n\"\n",
    "        f\"Content: {row['content']}\\n\\n\"\n",
    "        \"Images and their Alt Texts:\\n\"\n",
    "    )\n",
    "\n",
    "    for idx, image_info in enumerate(row['images']):\n",
    "        prompt += (\n",
    "            f\"Image {idx + 1} URL: {image_info['image_url']}\\n\"\n",
    "            f\"Image {idx + 1} Alt Text: {image_info['image_alt']}\\n\"\n",
    "        )\n",
    "\n",
    "    prompt += \"\\nPlease provide your ratings:\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an assistant that rates image importance in news articles.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    predicted_rating = response.choices[0].message.content.strip()\n",
    "    cleaned_rating = predicted_rating.replace('[', '').replace(']', '')\n",
    "    predicted_labels.append([int(x.strip()) for x in cleaned_rating.split(',')])\n",
    "\n",
    "\n",
    "test_df.loc[:, 'pred_labels'] = predicted_labels\n",
    "\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rmse(df, tolerance=0):\n",
    "    total_squared_errors = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        true_labels = row['true_labels']\n",
    "        pred_labels = row['pred_labels']\n",
    "        \n",
    "        if len(true_labels) != len(pred_labels):\n",
    "            raise ValueError(f\"Length of true_labels and pred_labels do not match for index {index}.\")\n",
    "\n",
    "        squared_errors = []\n",
    "        for true_label, pred_label in zip(true_labels, pred_labels):\n",
    "            difference = abs(pred_label - true_label)\n",
    "            error = max(0, difference - tolerance)\n",
    "            squared_errors.append(error ** 2)\n",
    "        \n",
    "        total_squared_errors.extend(squared_errors)\n",
    "\n",
    "    mse = np.mean(total_squared_errors)\n",
    "    rmse = np.sqrt(mse)\n",
    "    \n",
    "    return rmse / 4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_rmse(test_df, tolerance=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
