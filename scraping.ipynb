{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from openai import OpenAI\n",
    "import base64\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import json\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web scraping to download images per category in any given news website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_news_categories(url):\n",
    "    # news categories (and associated href) fetched via nav components\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching the URL: {e}\")\n",
    "        return []\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    categories = []\n",
    "    navs = soup.find_all('nav')\n",
    "\n",
    "    for nav in navs:\n",
    "        for link in nav.find_all('a'):\n",
    "            category = link.get_text(strip=True)\n",
    "            category_url = link.get('href')\n",
    "            if category and category_url:\n",
    "                categories.append((category, urljoin(url, category_url)))\n",
    "\n",
    "    return categories\n",
    "\n",
    "def create_directories(base_url, categories):\n",
    "    # create the following dir struct; outputs > base website > categories\n",
    "    base_dir = os.path.join(\"outputs\", urlparse(base_url).netloc)\n",
    "    if not os.path.exists(base_dir):\n",
    "        os.makedirs(base_dir)\n",
    "    \n",
    "    for category, _ in categories:\n",
    "        category_dir = os.path.join(base_dir, category)\n",
    "        if not os.path.exists(category_dir):\n",
    "            os.makedirs(category_dir)\n",
    "\n",
    "    return base_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_image(img_url, save_dir, img_name):\n",
    "    try:\n",
    "        if not img_url.startswith('data:'):\n",
    "            response = requests.get(img_url)\n",
    "            img_data = response.content\n",
    "            img = Image.open(BytesIO(img_data))\n",
    "            width, height = img.size\n",
    "\n",
    "            # Only save images larger than 100x100 pixels\n",
    "            if width >= 100 and height >= 100:\n",
    "                with open(os.path.join(save_dir, img_name), 'wb') as img_file:\n",
    "                    img_file.write(img_data)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "def download_images(category_url, save_dir):\n",
    "    # download all images of a particular category\n",
    "    try:\n",
    "        response = requests.get(category_url)\n",
    "        response.raise_for_status()\n",
    "    except Exception as e:\n",
    "        print(f'Error fetching the category URL: {e}')\n",
    "        return\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    images = soup.select('img[src]')\n",
    "\n",
    "    # parallising the downloads to make it faster\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        futures = []\n",
    "        for i, img in enumerate(images):\n",
    "            img_url = img.get('src')\n",
    "            if img_url and not img_url.startswith('data:'):\n",
    "                img_url = urljoin(category_url, img_url)\n",
    "                img_name = f'image_{i}.jpg'\n",
    "                futures.append(executor.submit(download_image, img_url, save_dir, img_name))\n",
    "        \n",
    "        for future in as_completed(futures):\n",
    "            future.result()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the base url to any news website for which you want to download images (for every category) \n",
    "\n",
    "Tried for: thegaurdian, time.com, tribune.pk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://www.washingtonpost.com/' #change this\n",
    "categories = get_news_categories(base_url)\n",
    "\n",
    "if not categories:\n",
    "    print(\"No categories found.\")\n",
    "    \n",
    "base_dir = create_directories(base_url, categories)\n",
    "for category, category_url in tqdm(categories, desc='Downloading images for every category'):\n",
    "    category_dir = os.path.join(base_dir, category)\n",
    "    download_images(category_url, category_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feeding images to GPT-4o (chat completion module) for their similarity scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we first supply the 'training' similarity scores in the form of a matrix for the images given in the train folder<br>The matrix for n images is an n x n *symmetric* matrix with (nC2) comparisons\n",
    "e.g,\n",
    "|       | img0  | img1  | \n",
    "|-------|-------|-------|\n",
    "| **img0** | 1.0   | x  | \n",
    "| **img1** | x  | 1.0   | \n",
    "\n",
    "\n",
    "This similarity matrix serves as the few shot example training for the LLM<br>Feel free to change/add to the training images in the train folder and redefine the training similarity matrix\n",
    "\n",
    "**Note: api calls charged per usage ~0.5 usd for every 5 image comparisons (5x5 simlarity matrix - 10 comparisons) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_number(filename):\n",
    "    match = re.search(r'image_(\\d+)', filename)\n",
    "    return int(match.group(1)) if match else float('inf')\n",
    "\n",
    "train_images = sorted(\n",
    "    [os.path.join('train', f) for f in os.listdir('train') if f.endswith(('jpg', 'jpeg', 'png'))],\n",
    "    key=extract_number\n",
    ")\n",
    "\n",
    "# define your similarity score labels for training\n",
    "similarity_scores = [\n",
    "    [1.0, 0.7, 0.3, 0.3],\n",
    "    [0.7, 1.0, 0.4, 0.4],\n",
    "    [0.3, 0.4, 1.0, 0.9],\n",
    "    [0.3, 0.4, 0.9, 1.0]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "openai_api_key = os.getenv('api_key')\n",
    "api_key = openai_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### You can find the api key on the slack channel or use your own api key\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "def load_image_as_base64(image_path):\n",
    "    with open(image_path, 'rb') as img_file:\n",
    "        return base64.b64encode(img_file.read()).decode('utf-8')\n",
    "\n",
    "few_shot_examples = []\n",
    "\n",
    "# create few shot training with sample question answering\n",
    "for i in range(len(train_images)):\n",
    "    for j in range(i + 1, len(train_images)):\n",
    "        base64_img1 = load_image_as_base64(train_images[i])\n",
    "        base64_img2 = load_image_as_base64(train_images[j])\n",
    "        \n",
    "        few_shot_examples.extend([\n",
    "            {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_img1}\"}\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_img2}\"}\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": f\"Question: Compute semantic similarity score for the above pair of images.\\nAnswer: {similarity_scores[i][j]}\"\n",
    "            }\n",
    "        ])\n",
    "\n",
    "# system role to specify the answer content - feel free to prompt engineer here\n",
    "system_message = [\n",
    "    {\"role\": \"system\", \"content\": \"You need to assign similarity scores between 0 and 1 to pairs of images based on the main content and context of the image focusing on actions, emotions, and overall meaning and NOT on specific visual details such as colors or specific objects. Your response should ONLY contain the similairty score number (no words/phrases). Follow the examples below:\"},\n",
    "    {\"role\": \"user\", \"content\": few_shot_examples}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_images(image_paths):\n",
    "    # compute similarity scores given the prompt above for all combinations of all images passed\n",
    "    user_messages = []\n",
    "    \n",
    "    for i in range(len(image_paths)):\n",
    "        for j in range(i + 1, len(image_paths)):\n",
    "            base64_img1 = load_image_as_base64(image_paths[i])\n",
    "            base64_img2 = load_image_as_base64(image_paths[j])\n",
    "            \n",
    "            user_messages.append({\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": \"Compare these two images for semantic similarity.\"},\n",
    "                    {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_img1}\"}},\n",
    "                    {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_img2}\"}}\n",
    "                ]\n",
    "            })\n",
    "\n",
    "    responses = []\n",
    "    for message in tqdm(user_messages, desc=\"Processing image pairs\"):\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=system_message + [message],\n",
    "            max_tokens=300\n",
    "        )\n",
    "        time.sleep(1)\n",
    "        res = response.choices[0].message.content\n",
    "        score = re.findall(r'-?\\d*\\.?\\d+', res)[0]\n",
    "        print(score)\n",
    "        responses.append(score)\n",
    "    \n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_matrix(image_paths, similarity_scores):\n",
    "    n = len(image_paths)\n",
    "    similarity_matrix = [[1.0 if i == j else 0.0 for j in range(n)] for i in range(n)]\n",
    "\n",
    "    idx = 0\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            similarity_matrix[i][j] = float(similarity_scores[idx])\n",
    "            similarity_matrix[j][i] = float(similarity_scores[idx])\n",
    "            idx += 1\n",
    "\n",
    "    return similarity_matrix\n",
    "\n",
    "def process_categories(base_dir):\n",
    "    category_folders = [d for d in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, d))]\n",
    "\n",
    "    for category_dir in category_folders:\n",
    "        category_path = os.path.join(base_dir, category_dir)\n",
    "        image_paths = sorted(\n",
    "            [os.path.join(category_path, f) for f in os.listdir(category_path) if f.endswith(('jpg', 'jpeg', 'png'))],\n",
    "            key=extract_number\n",
    "        )\n",
    "        if len(image_paths) < 2:\n",
    "            continue\n",
    "\n",
    "        similarity_scores = compare_images(image_paths)\n",
    "        similarity_matrix = make_matrix(image_paths, similarity_scores)\n",
    "\n",
    "        with open(f'pred_labels/with_fewshot/{category_dir}.csv', 'w', newline='') as csv_file:\n",
    "            writer = csv.writer(csv_file)\n",
    "            writer.writerows(similarity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_matrix_from_csv(filepath):\n",
    "    with open(filepath, 'r') as csv_file:\n",
    "        reader = csv.reader(csv_file)\n",
    "        matrix = [list(map(float, row)) for row in reader]\n",
    "    return np.array(matrix)\n",
    "\n",
    "def compute_rmse(pred_folder, labels_folder, tolerance=0.1):\n",
    "    test_files = sorted([f for f in os.listdir(pred_folder) if f.endswith('.csv')])\n",
    "    labels_files = sorted([f for f in os.listdir(labels_folder) if f.endswith('.csv')])\n",
    "\n",
    "    if len(test_files) != len(labels_files):\n",
    "        raise ValueError(\"The number of files in test_folder and labels_folder does not match.\")\n",
    "\n",
    "    total_squared_errors = []\n",
    "\n",
    "    for test_file in test_files:\n",
    "        label_file = test_file\n",
    "        test_path = os.path.join(pred_folder, test_file)\n",
    "        label_path = os.path.join(labels_folder, label_file)\n",
    "\n",
    "        if not os.path.isfile(test_path) or not os.path.isfile(label_path):\n",
    "            raise ValueError(f\"Corresponding file for '{test_file}' is missing in one of the folders.\")\n",
    "\n",
    "        test_matrix = read_matrix_from_csv(test_path)\n",
    "        label_matrix = read_matrix_from_csv(label_path)\n",
    "\n",
    "        if test_matrix.shape != label_matrix.shape:\n",
    "            raise ValueError(f\"Matrix shapes for '{test_file}' do not match.\")\n",
    "\n",
    "        squared_errors = []\n",
    "        for row_test, row_label in zip(test_matrix, label_matrix):\n",
    "            for score_test, score_label in zip(row_test, row_label):\n",
    "                difference = abs(score_test - score_label)\n",
    "                error = max(0, difference - tolerance)\n",
    "                squared_errors.append(error ** 2)\n",
    "\n",
    "        total_squared_errors.extend(squared_errors)\n",
    "\n",
    "    mse = np.mean(total_squared_errors)\n",
    "    rmse = np.sqrt(mse)\n",
    "    return rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_matrix_to_csv(output_file, avg_matrix):\n",
    "    with open(output_file, 'w', newline='') as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        writer.writerows(avg_matrix)\n",
    "\n",
    "def compute_average_matrix(matrix1, matrix2):\n",
    "    return np.round((matrix1 + matrix2) / 2, 1)\n",
    "\n",
    "def extract_base_filename(filename):\n",
    "    base_name, _ = os.path.splitext(filename) \n",
    "    return re.sub(r'\\d+$', '', base_name)\n",
    "\n",
    "def process_labels(base_dir):\n",
    "    for category in os.listdir(base_dir):\n",
    "        category_path = os.path.join(base_dir, category)\n",
    "        \n",
    "        if not os.path.isdir(category_path):\n",
    "            continue\n",
    "        \n",
    "        files = sorted([f for f in os.listdir(category_path) if f.endswith('.csv')])\n",
    "\n",
    "        for i in range(0, len(files), 2):\n",
    "            file1 = files[i]\n",
    "            file2 = files[i+1] if i+1 < len(files) else None\n",
    "            \n",
    "            if file2 is None:\n",
    "                continue\n",
    "            \n",
    "            matrix1 = read_matrix_from_csv(os.path.join(category_path, file1))\n",
    "            matrix2 = read_matrix_from_csv(os.path.join(category_path, file2))\n",
    "            \n",
    "            avg_matrix = compute_average_matrix(matrix1, matrix2)\n",
    "\n",
    "            output_file = f'true_labels/{category}_{extract_base_filename(file1)}.csv'\n",
    "            os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "            \n",
    "            write_matrix_to_csv(output_file, avg_matrix)\n",
    "    \n",
    "    print('Average similarity matrices(labels) written to files')\n",
    "\n",
    "process_labels('true_labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the similairty matrix for each category in the test folder and saving the results to test.json<br>\n",
    "Feel free to add to / remove from the test folder - **I first generate images using the web scraping module above and then only pick 2-3 categories with 4-5 images each to test because of the cost of api usage**\n",
    "\n",
    "***** Please only run the below cell for *new* images added to the test folder because previous results have already been computed and kept in the test.json file - rerunning for the same images will only cost more without any benefit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = 'test'\n",
    "process_categories(test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total RMSE: 0.10996211468804457\n"
     ]
    }
   ],
   "source": [
    "rmse = compute_rmse(pred_folder = 'pred_labels/with_fewshot', labels_folder = 'true_labels', tolerance=0.1)\n",
    "print(f'Total RMSE: {rmse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category: washington\n",
      "Inter-rater reliability (ICC): 0.38\n",
      "Category: bbc\n",
      "Inter-rater reliability (ICC): 0.46\n"
     ]
    }
   ],
   "source": [
    "import pingouin as pg\n",
    "\n",
    "def flatten_upper_triangle(matrix):\n",
    "    return matrix[np.triu_indices_from(matrix, 1)]\n",
    "\n",
    "def icc(flat_matrices_by_rater):\n",
    "    df = pd.DataFrame(flat_matrices_by_rater)\n",
    "    \n",
    "    df['Target'] = df.index \n",
    "    df_long = pd.melt(df, id_vars='Target', var_name='Rater', value_name='Similarity')\n",
    "    \n",
    "    icc_results = pg.intraclass_corr(data=df_long, targets='Target', raters='Rater', ratings='Similarity')\n",
    "    icc_value = icc_results.iloc[0]['ICC']\n",
    "    \n",
    "    return round(icc_value, 2)\n",
    "\n",
    "def find_icc_per_category(base_dir):\n",
    "    for category in os.listdir(base_dir):\n",
    "        category_path = os.path.join(base_dir, category)\n",
    "        \n",
    "        if not os.path.isdir(category_path):\n",
    "            continue\n",
    "        \n",
    "        files = sorted([f for f in os.listdir(category_path) if f.endswith('.csv')])\n",
    "\n",
    "        matrices_by_rater = {}\n",
    "        \n",
    "        for file in files:\n",
    "            filename, _ = os.path.splitext(file)\n",
    "            rater = filename[-1]\n",
    "            \n",
    "            if rater not in matrices_by_rater:\n",
    "                matrices_by_rater[rater] = []\n",
    "            \n",
    "            matrix = read_matrix_from_csv(os.path.join(category_path, file))\n",
    "            matrices_by_rater[rater].append(matrix)\n",
    "        \n",
    "        flat_matrices_by_rater = {}\n",
    "        \n",
    "        for rater, matrices in matrices_by_rater.items():\n",
    "            flat_matrices = [flatten_upper_triangle(matrix) for matrix in matrices]\n",
    "            flat_data = np.concatenate(flat_matrices)\n",
    "            flat_matrices_by_rater[f'Rater_{rater}'] = flat_data\n",
    "        \n",
    "        if len(flat_matrices_by_rater) > 1:\n",
    "            inter_rater_compatibility = icc(flat_matrices_by_rater)\n",
    "            print(f'Category: {category}')\n",
    "            print(f'Inter-rater reliability (ICC): {inter_rater_compatibility}')\n",
    "        else:\n",
    "            print(f'Not enough matrices to compute ICC for category: {category}')\n",
    "\n",
    "find_icc_per_category('true_labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Value Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WORK IN PROGRESS\n",
    "\n",
    "def sanitize_filename(filename):\n",
    "    return \"\".join(c for c in filename if c.isalnum() or c in (' ', '.', '_')).rstrip()\n",
    "\n",
    "def get_article_links(page_url):\n",
    "    try:\n",
    "        response = requests.get(page_url)\n",
    "        response.raise_for_status()\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching page URL: {e}\")\n",
    "        return []\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    article_links = []\n",
    "\n",
    "    for link in soup.find_all('a', href=True):\n",
    "        href = link['href']\n",
    "        if 'article' in href or '/news/' in href:\n",
    "            article_links.append(urljoin(page_url, href))\n",
    "\n",
    "    return article_links\n",
    "\n",
    "def download_article_content(article_url, save_dir):\n",
    "    try:\n",
    "        response = requests.get(article_url)\n",
    "        response.raise_for_status()\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching article URL: {e}\")\n",
    "        return\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    title = soup.find('title').get_text(strip=True) if soup.find('title') else 'Untitled'\n",
    "    title_sanitized = sanitize_filename(title)\n",
    "    article_dir = os.path.join(save_dir, title_sanitized)\n",
    "\n",
    "    if not os.path.exists(article_dir):\n",
    "        os.makedirs(article_dir)\n",
    "\n",
    "    with open(os.path.join(article_dir, 'title.txt'), 'w', encoding='utf-8') as f:\n",
    "        f.write(title)\n",
    "\n",
    "    images = soup.select('img[src]')\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        futures = []\n",
    "        for i, img in enumerate(images):\n",
    "            img_url = img.get('src')\n",
    "            if img_url:\n",
    "                img_url = urljoin(article_url, img_url)\n",
    "                img_name = f'image_{i + 1}.jpg'\n",
    "                futures.append(executor.submit(download_image, img_url, article_dir, img_name))\n",
    "        \n",
    "        for future in as_completed(futures):\n",
    "            future.result()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
