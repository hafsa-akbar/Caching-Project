{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from openai import OpenAI\n",
    "import base64\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import json\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web scraping to download images per category in any given news website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_news_categories(url):\n",
    "    # news categories (and associated href) fetched via nav components\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching the URL: {e}\")\n",
    "        return []\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    categories = []\n",
    "    navs = soup.find_all('nav')\n",
    "\n",
    "    for nav in navs:\n",
    "        for link in nav.find_all('a'):\n",
    "            category = link.get_text(strip=True)\n",
    "            category_url = link.get('href')\n",
    "            if category and category_url:\n",
    "                categories.append((category, urljoin(url, category_url)))\n",
    "\n",
    "    return categories\n",
    "\n",
    "def create_directories(base_url, categories):\n",
    "    # create the following dir struct; outputs > base website > categories\n",
    "    base_dir = os.path.join(\"outputs\", urlparse(base_url).netloc)\n",
    "    if not os.path.exists(base_dir):\n",
    "        os.makedirs(base_dir)\n",
    "    \n",
    "    for category, _ in categories:\n",
    "        category_dir = os.path.join(base_dir, category)\n",
    "        if not os.path.exists(category_dir):\n",
    "            os.makedirs(category_dir)\n",
    "\n",
    "    return base_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_image(img_url, save_dir, img_name):\n",
    "    try:\n",
    "        if img_url.startswith('data:'):\n",
    "            save_data_uri_image(img_url, save_dir, img_name)\n",
    "        else:\n",
    "            save_url_image(img_url, save_dir, img_name)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'Error downloading image {img_url}: {e}')\n",
    "\n",
    "def get_extension_from_header(header):\n",
    "    if \"image/jpeg\" in header:\n",
    "        return \".jpg\"\n",
    "    elif \"image/png\" in header:\n",
    "        return \".png\"\n",
    "    elif \"image/xml\" in header:\n",
    "        return \".xml\"\n",
    "    return None\n",
    "\n",
    "def save_data_uri_image(img_url, save_dir, img_name):\n",
    "    header, encoded = img_url.split(',', 1)\n",
    "    data = base64.b64decode(encoded)\n",
    "    ext = get_extension_from_header(header)\n",
    "    if ext:\n",
    "        img_name = img_name.split('.')[0] + ext\n",
    "        img_path = os.path.join(save_dir, img_name)\n",
    "        with open(img_path, 'wb') as f:\n",
    "            f.write(data)\n",
    "\n",
    "def save_url_image(img_url, save_dir, img_name):\n",
    "    response = requests.get(img_url)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    img_path = os.path.join(save_dir, img_name)\n",
    "    with open(img_path, 'wb') as f:\n",
    "        f.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_article(link_tup, base_url, category_url, save_dir, csv_writer):\n",
    "    article_number, link = link_tup\n",
    "\n",
    "    article_url = urljoin(category_url, link.get('href'))\n",
    "    try:\n",
    "        article_response = requests.get(article_url)\n",
    "        article_response.raise_for_status()\n",
    "\n",
    "        article_soup = BeautifulSoup(article_response.content, 'html.parser')\n",
    "        article_heading = extract_heading(link, article_soup)\n",
    "        \n",
    "        images = article_soup.find_all('img', src=True)\n",
    "        ############# SPECIFIC FOR WEBSITE #####################\n",
    "        images_first_filter = [\n",
    "                img for img in images\n",
    "                if img.find_parent('main', class_='article__main') and\n",
    "                not img.find_parent('a', class_='related-content__link') and\n",
    "                not img.find_parent('div', class_='video-inline_carousel__wrapper')\n",
    "        ]\n",
    "        ############# SPECIFIC FOR WEBSITE #####################\n",
    "\n",
    "        images_to_download = filter_images(images_first_filter)\n",
    "        download_and_record_images(article_number, images_to_download, base_url, article_url, save_dir, csv_writer, article_heading)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'Error processing the article {article_url}: {e}')\n",
    "\n",
    "def extract_heading(link, soup):\n",
    "    heading = link.get('aria-label')\n",
    "    if not heading:\n",
    "        heading = soup.find('h1').get_text(strip=True)\n",
    "    if not heading:\n",
    "        heading = soup.find('h2').get_text(strip=True)\n",
    "    if not heading:\n",
    "        meta_title = soup.find('meta', attrs={'property': 'og:title'}).get_text(strip=True)\n",
    "        if meta_title:\n",
    "            return meta_title.get('content').strip()\n",
    "    if not heading:\n",
    "        heading = soup.find('title').get_text(strip=True)\n",
    "\n",
    "    return heading if heading else 'No Heading'\n",
    "\n",
    "def download_and_record_images(article_number, images_to_download, base_url, article_url, save_dir, csv_writer, article_heading):\n",
    "    count=0\n",
    "    for img in images_to_download:\n",
    "        img_url = urljoin(article_url, img['src'])\n",
    "\n",
    "        img_name = f'image_{article_number}_{count}.jpg'\n",
    "        count+=1\n",
    "\n",
    "        alt_text = img.get('alt', '')\n",
    "\n",
    "        download_image(img_url, save_dir, img_name)\n",
    "        csv_writer.writerow([base_url, os.path.basename(save_dir), article_number, count, alt_text, article_heading, article_url])\n",
    "        \n",
    "        time.sleep(0.001)\n",
    "\n",
    "processed_articles = set()  \n",
    "def download_images(base_url, category_url, save_dir, csv_writer):\n",
    "    response = requests.get(category_url)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    ##################### GET ARTICLE LINKS SPECIFIC TO WEBSITE ###################\n",
    "    def extract_date_from_url(a_tag):\n",
    "        url = a_tag['href']\n",
    "        try:\n",
    "            date_part = url.split(\"/\")[1:4] \n",
    "            date_str = \"-\".join(date_part)\n",
    "            return datetime.strptime(date_str, \"%Y-%m-%d\")\n",
    "        except:\n",
    "            return datetime.min\n",
    "    \n",
    "    articles = set()\n",
    "    div = soup.find('section', class_='layout__main layout-no-rail__main')\n",
    "    for link in div.find_all('a', href=True):\n",
    "        if '/2024/' in link['href'] and link['href'] not in processed_articles:  \n",
    "            articles.add(link)\n",
    "            processed_articles.add(link['href'])\n",
    "    \n",
    "    sorted_links = sorted(articles, key=extract_date_from_url, reverse=True)\n",
    "    article_dict = {i+1: link for i, link in enumerate(sorted_links[:11])}  # Limit to 10\n",
    "    ####################################################################################\n",
    "\n",
    "    for link_dict in article_dict.items():\n",
    "        process_article(link_dict, base_url, category_url, save_dir, csv_writer)\n",
    "        _, link = link_dict\n",
    "\n",
    "def filter_images(images):\n",
    "    images_to_download = []\n",
    "    for img in images:\n",
    "        width = img.get('width')\n",
    "        img_url = img.get('src', '')\n",
    "\n",
    "        if not width:\n",
    "            width_from_url = re.search(r'width=(\\d+)', img_url)\n",
    "            \n",
    "            if width_from_url:\n",
    "                width = width_from_url.group(1)\n",
    "            \n",
    "            if not width:\n",
    "                continue\n",
    "\n",
    "        width_is_large = ('%' in width and float(width.replace('%', '')) > 60) or (width.isdigit() and float(width) > 250)\n",
    "\n",
    "        if width_is_large:\n",
    "            images_to_download.append(img)\n",
    "\n",
    "    return images_to_download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_csv(base_dir):\n",
    "    csv_file_path = os.path.join(base_dir, 'image_data.csv')\n",
    "    csv_file = open(csv_file_path, mode='w', newline='', encoding='utf-8')\n",
    "\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "    csv_writer.writerow(['website', 'category', 'article_number', 'image number', 'alt', 'article_heading', 'article_url'])\n",
    "    \n",
    "    return csv_file, csv_writer\n",
    "\n",
    "def filter_categories(categories, exclude_keywords):\n",
    "    filtered_categories = []\n",
    "    for category, category_url in categories:\n",
    "        category_words = category.lower().split()\n",
    "        if not any(keyword in category_words for keyword in exclude_keywords):\n",
    "            filtered_categories.append((category, category_url))\n",
    "    return filtered_categories\n",
    "\n",
    "exclude_keywords = ['sign', 'login', 'subscribe', 'advertisement', 'privacy', 'terms', 'contact']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading images for each category: 100%|██████████| 5/5 [01:17<00:00, 15.53s/it]\n"
     ]
    }
   ],
   "source": [
    "categories = [\n",
    "    (\"business\", \"https://edition.cnn.com/business\"),\n",
    "    (\"sports\", \"https://edition.cnn.com/sport\"),\n",
    "    (\"world\", \"https://edition.cnn.com/world\"),\n",
    "    (\"entertainment\", \"https://edition.cnn.com/entertainment\"),\n",
    "    (\"science\", \"https://edition.cnn.com/science\")\n",
    "]\n",
    "\n",
    "base_url = \"https://edition.cnn.com\"\n",
    "base_dir = create_directories(base_url, categories)\n",
    "\n",
    "csv_file, csv_writer = setup_csv(base_dir)\n",
    "\n",
    "for category, category_url in tqdm(categories, desc='Downloading images for each category'):\n",
    "    category_dir = os.path.join(base_dir, category.replace('/', '_'))\n",
    "    os.makedirs(category_dir, exist_ok=True)\n",
    "    download_images(base_url, category_url, category_dir, csv_writer)\n",
    "    \n",
    "csv_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/2024/09/05/tech/verizon-frontier-deal/index.html\n",
      "/2024/09/05/business/burberry-ftse-100-drop/index.html\n",
      "/2024/09/05/business/trump-economy-tariffs/index.html\n",
      "/2024/09/05/tech/nvidia-stock-falling-nightcap/index.html\n",
      "/2024/09/04/tech/from-the-river-to-the-sea-meta-oversight-board/index.html\n",
      "/2024/09/04/business/chipotle-spirit-halloween-costume/index.html\n",
      "/2024/09/04/business/cpsc-shein-temu-investigation/index.html\n",
      "/2024/09/04/business/us-steel-nippon-steel-chances/index.html\n",
      "/2024/09/04/economy/us-jolts-job-openings-hires-layoffs-july/index.html\n",
      "/2024/09/04/economy/economy-jobs-report-harris-trump-nightcap/index.html\n",
      "/2024/09/04/tech/brazil-lula-elon-musk-x-suspension-hnk-intl/index.html\n",
      "/2024/09/04/business/ll-flooring-liquidation/index.html\n",
      "/2024/09/04/business/hotel-strike-san-diego/index.html\n",
      "/2024/09/04/business/dollar-stores-walmart-low-income-consumers/index.html\n",
      "/2024/09/04/tech/nvidia-is-in-trouble/index.html\n"
     ]
    }
   ],
   "source": [
    "def download_images(category_url):\n",
    "    \n",
    "    response = requests.get(category_url)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    ##################### GET ARTICLE LINKS SPECIFIC TO WEBSITE ###################\n",
    "    def extract_date_from_url(url):\n",
    "        try:\n",
    "            date_part = url.split(\"/\")[1:4] \n",
    "            date_str = \"-\".join(date_part)\n",
    "            return datetime.strptime(date_str, \"%Y-%m-%d\")\n",
    "        except:\n",
    "            return datetime.min\n",
    "    \n",
    "    articles = set()\n",
    "    div = soup.find('section', class_='layout__main layout-no-rail__main')\n",
    "    for link in div.find_all('a', href=True):\n",
    "        if '/2024/' in link['href']:  \n",
    "            articles.add(link['href'])\n",
    "    \n",
    "    sorted_links = sorted(articles, key=extract_date_from_url, reverse=True)\n",
    "    article_links = sorted_links[0:15]  # Limit to first 15 articles for each category\n",
    "    ####################################################################################\n",
    "    for link in article_links:\n",
    "        print(link)\n",
    "\n",
    "download_images(\"https://edition.cnn.com/business\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folders missing images\n",
    "main_dir = os.path.join(os.getcwd(), 'outputs/theguardian.com')\n",
    "total = 0\n",
    "for dir in os.listdir(main_dir):\n",
    "    full_path = os.path.join(main_dir, dir)  \n",
    "    if os.path.isdir(full_path):\n",
    "        if len(os.listdir(full_path)) == 0:\n",
    "            total+=1\n",
    "            print(dir)\n",
    "\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the base url to any news website for which you want to download images (for every category) \n",
    "\n",
    "Tried for: thegaurdian, time.com, tribune.pk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feeding images to GPT-4o (chat completion module) for their similarity scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we first supply the 'training' similarity scores in the form of a matrix for the images given in the train folder<br>The matrix for n images is an n x n *symmetric* matrix with (nC2) comparisons\n",
    "e.g,\n",
    "|       | img0  | img1  | \n",
    "|-------|-------|-------|\n",
    "| **img0** | 1.0   | x  | \n",
    "| **img1** | x  | 1.0   | \n",
    "\n",
    "\n",
    "This similarity matrix serves as the few shot example training for the LLM<br>Feel free to change/add to the training images in the train folder and redefine the training similarity matrix\n",
    "\n",
    "**Note: api calls charged per usage ~0.5 usd for every 5 image comparisons (5x5 simlarity matrix - 10 comparisons) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "openai_api_key = os.getenv('api_key')\n",
    "api_key = openai_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_number(filename):\n",
    "    match = re.search(r'image_(\\d+)', filename)\n",
    "    return int(match.group(1)) if match else float('inf')\n",
    "\n",
    "def load_image_as_base64(image_path):\n",
    "    with open(image_path, 'rb') as img_file:\n",
    "        return base64.b64encode(img_file.read()).decode('utf-8')\n",
    "\n",
    "def read_matrix_from_csv(filepath):\n",
    "    with open(filepath, 'r') as csv_file:\n",
    "        reader = csv.reader(csv_file)\n",
    "        matrix = [list(map(float, row)) for row in reader]\n",
    "    return np.array(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "def find_similar_category(category, train_dir='train'):\n",
    "    categories = [d for d in os.listdir(train_dir) if os.path.isdir(os.path.join(train_dir, d))]\n",
    "    prompt = f\"Which of these categories: {', '.join(categories)} is most similar to '{category}'? Respond ONLY with the category name without any additional words or punctuation.\"\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\", \n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            max_tokens=10 \n",
    "        )\n",
    "        similar_category = response.choices[0].message.content.strip()\n",
    "        return similar_category\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in finding similar category: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def few_shot(category, static_folder=None):\n",
    "    if static_folder is None:\n",
    "        similar_category = find_similar_category(category)\n",
    "        \n",
    "        images_path = os.path.join('train', similar_category)\n",
    "    else:\n",
    "        images_path = static_folder\n",
    "\n",
    "    labels_csv = os.path.join(images_path, 'labels.csv')\n",
    "    similarity_csv = os.path.join(images_path, 'similarity.csv')\n",
    "\n",
    "    image_paths = sorted(\n",
    "        [os.path.join(images_path, f) for f in os.listdir(images_path) if f.endswith(('jpg', 'jpeg', 'png'))],\n",
    "        key=extract_number\n",
    "    )\n",
    "    \n",
    "    labels = pd.read_csv(labels_csv)\n",
    "    similarity_scores = read_matrix_from_csv(similarity_csv)\n",
    "    few_shot_examples = create_examples(image_paths, labels, similarity_scores)\n",
    "    \n",
    "    return few_shot_examples\n",
    "\n",
    "def create_examples(image_paths, labels, similarity_scores):\n",
    "    few_shot_examples = []\n",
    "    for i in range(len(image_paths)):\n",
    "        for j in range(i + 1, len(image_paths)):\n",
    "            base64_img1 = load_image_as_base64(image_paths[i])\n",
    "            base64_img2 = load_image_as_base64(image_paths[j])\n",
    "            \n",
    "            img1_number = extract_number(image_paths[i])\n",
    "            img2_number = extract_number(image_paths[j])\n",
    "            \n",
    "            alt_text1 = labels.loc[labels['image number'] == img1_number, 'alt'].values[0]\n",
    "            heading1 = labels.loc[labels['image number'] == img1_number, 'article_heading'].values[0]\n",
    "            alt_text2 = labels.loc[labels['image number'] == img2_number, 'alt'].values[0]\n",
    "            heading2 = labels.loc[labels['image number'] == img2_number, 'article_heading'].values[0]\n",
    "\n",
    "            few_shot_examples.append({\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": f\"Image 1 Alt Text: {alt_text1}\\nImage 1 Heading: {heading1}\"\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_img1}\"}\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": f\"Image 2 Alt Text: {alt_text2}\\nImage 2 Heading: {heading2}\"\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_img2}\"}\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": f\"Question: Compute semantic similarity score (on 0-4 scale) for the above pair of images,\\\n",
    "                        considering the content, context, alt text, and article headings i.e, how replaceable is one image \\\n",
    "                        with the another (0-4 scale). \\nAnswer: {similarity_scores[i][j]}\"\n",
    "                }]\n",
    "            })\n",
    "    return few_shot_examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_images(image_paths, labels, few_shot_examples):\n",
    "    user_messages = []\n",
    "\n",
    "    for i in range(len(image_paths)):\n",
    "        for j in range(i + 1, len(image_paths)):\n",
    "            base64_img1 = load_image_as_base64(image_paths[i])\n",
    "            base64_img2 = load_image_as_base64(image_paths[j])\n",
    "            \n",
    "            img1_number = extract_number(image_paths[i])\n",
    "            img2_number = extract_number(image_paths[j])\n",
    "            \n",
    "            alt_text1 = labels.loc[labels['image number'] == img1_number, 'alt'].values[0]\n",
    "            heading1 = labels.loc[labels['image number'] == img1_number, 'article_heading'].values[0]\n",
    "            alt_text2 = labels.loc[labels['image number'] == img2_number, 'alt'].values[0]\n",
    "            heading2 = labels.loc[labels['image number'] == img2_number, 'article_heading'].values[0]\n",
    "\n",
    "            user_messages.append({\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": f\"Image 1 Alt Text: {alt_text1}\\nImage 1 Heading: {heading1}\"},\n",
    "                    {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_img1}\"}},\n",
    "\n",
    "                    {\"type\": \"text\", \"text\": f\"Image 2 Alt Text: {alt_text2}\\nImage 2 Heading: {heading2}\"},\n",
    "                    {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_img2}\"}},\n",
    "\n",
    "                    {\"type\": \"text\",\n",
    "                    \"text\": \"Question: On a scale from 0 to 4 (0: Not replaceable, 1: Somewhat replaceable, 2: Moderately replaceable, \\\n",
    "                        3: Very replaceable, 4: Completely replaceable), rate the similarity of these two images based on the images themselves, \\\n",
    "                        their alt text descriptions, and the article headings where they are used. Please respond with ONLY the numerical score, \\\n",
    "                        without any additional text or punctuation.\"}\n",
    "                ]\n",
    "            })\n",
    "\n",
    "    responses = []\n",
    "    for message in tqdm(user_messages, desc=\"Processing image pairs\"):\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=few_shot_examples + [message],\n",
    "            max_tokens=10\n",
    "        )\n",
    "        time.sleep(1)\n",
    "        res = response.choices[0].message.content.strip()\n",
    "        score = re.findall(r'-?\\d*\\.?\\d+', res)[0] \n",
    "        responses.append(score)\n",
    "    \n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_matrix(image_paths, similarity_scores):\n",
    "    n = len(image_paths)\n",
    "    similarity_matrix = [[4 if i == j else 0 for j in range(n)] for i in range(n)]\n",
    "\n",
    "    idx = 0\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            similarity_matrix[i][j] = int(similarity_scores[idx])\n",
    "            similarity_matrix[j][i] = int(similarity_scores[idx])\n",
    "            idx += 1\n",
    "\n",
    "    return similarity_matrix\n",
    "\n",
    "def process_categories(base_dir, static_folder=None, zero_shot=False):\n",
    "    category_folders = [d for d in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, d))]\n",
    "\n",
    "    for category in category_folders:\n",
    "        category_path = os.path.join(base_dir, category)\n",
    "        labels = pd.read_csv(os.path.join(category_path, 'labels.csv'))\n",
    "        \n",
    "        image_paths = sorted(\n",
    "            [os.path.join(category_path, f) for f in os.listdir(category_path) if f.endswith(('jpg', 'jpeg', 'png'))],\n",
    "            key=extract_number\n",
    "        )\n",
    "        if len(image_paths) < 2:\n",
    "            continue\n",
    "\n",
    "        few_shot_examples = []\n",
    "        if not zero_shot:\n",
    "            few_shot_examples = few_shot(category, static_folder)\n",
    "            \n",
    "        similarity_scores = compare_images(image_paths, labels, few_shot_examples)\n",
    "        similarity_matrix = make_matrix(image_paths, similarity_scores)\n",
    "\n",
    "        output_file = 'with_fewshot' if not zero_shot else 'without_fewshot'\n",
    "\n",
    "        with open(f'pred_labels/{output_file}/{category}.csv', 'w', newline='') as csv_file:\n",
    "            writer = csv.writer(csv_file)\n",
    "            writer.writerows(similarity_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions to compute RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_matrix_from_csv(filepath):\n",
    "    with open(filepath, 'r') as csv_file:\n",
    "        reader = csv.reader(csv_file)\n",
    "        matrix = [list(map(float, row)) for row in reader]\n",
    "    return np.array(matrix)\n",
    "\n",
    "def compute_rmse(pred_folder, labels_folder, tolerance=0):\n",
    "    test_files = sorted([f for f in os.listdir(pred_folder) if f.endswith('.csv')])\n",
    "    labels_files = sorted([f for f in os.listdir(labels_folder) if f.endswith('.csv')])\n",
    "\n",
    "    if len(test_files) != len(labels_files):\n",
    "        raise ValueError(\"The number of files in test_folder and labels_folder does not match.\")\n",
    "\n",
    "    total_squared_errors = []\n",
    "\n",
    "    for test_file in test_files:\n",
    "        label_file = test_file\n",
    "        test_path = os.path.join(pred_folder, test_file)\n",
    "        label_path = os.path.join(labels_folder, label_file)\n",
    "\n",
    "        if not os.path.isfile(test_path) or not os.path.isfile(label_path):\n",
    "            raise ValueError(f\"Corresponding file for '{test_file}' is missing in one of the folders.\")\n",
    "\n",
    "        test_matrix = read_matrix_from_csv(test_path)\n",
    "        label_matrix = read_matrix_from_csv(label_path)\n",
    "\n",
    "        if test_matrix.shape != label_matrix.shape:\n",
    "            raise ValueError(f\"Matrix shapes for '{test_file}' do not match.\")\n",
    "\n",
    "        squared_errors = []\n",
    "        for row_test, row_label in zip(test_matrix, label_matrix):\n",
    "            for score_test, score_label in zip(row_test, row_label):\n",
    "                difference = abs(score_test - score_label)\n",
    "                error = max(0, difference - tolerance)\n",
    "                squared_errors.append(error ** 2)\n",
    "\n",
    "        total_squared_errors.extend(squared_errors)\n",
    "\n",
    "    mse = np.mean(total_squared_errors)\n",
    "    rmse = np.sqrt(mse)\n",
    "    return rmse/4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute Average of True Labels between two raters and ICC score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average similarity matrices(labels) written to files\n"
     ]
    }
   ],
   "source": [
    "def write_matrix_to_csv(output_file, avg_matrix):\n",
    "    with open(output_file, 'w', newline='') as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        writer.writerows(avg_matrix.astype(int))\n",
    "\n",
    "def compute_average_matrix(matrix1, matrix2):\n",
    "    return np.round((matrix1 + matrix2) / 2, 0)\n",
    "\n",
    "def extract_base_filename(filename):\n",
    "    base_name, _ = os.path.splitext(filename) \n",
    "    return re.sub(r'\\d+$', '', base_name)\n",
    "\n",
    "def process_labels(base_dir):\n",
    "    for category in os.listdir(base_dir):\n",
    "        category_path = os.path.join(base_dir, category)\n",
    "        \n",
    "        if not os.path.isdir(category_path):\n",
    "            continue\n",
    "        \n",
    "        files = sorted([f for f in os.listdir(category_path) if f.endswith('.csv')])\n",
    "\n",
    "        for i in range(0, len(files), 2):\n",
    "            file1 = files[i]\n",
    "            file2 = files[i+1] if i+1 < len(files) else None\n",
    "            \n",
    "            if file2 is None:\n",
    "                continue\n",
    "            \n",
    "            matrix1 = read_matrix_from_csv(os.path.join(category_path, file1))\n",
    "            matrix2 = read_matrix_from_csv(os.path.join(category_path, file2))\n",
    "            \n",
    "            avg_matrix = compute_average_matrix(matrix1, matrix2)\n",
    "\n",
    "            output_file = f'true_labels/{category}_{extract_base_filename(file1)}.csv'\n",
    "            os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "            \n",
    "            write_matrix_to_csv(output_file, avg_matrix)\n",
    "    \n",
    "    print('Average similarity matrices(labels) written to files')\n",
    "\n",
    "process_labels('true_labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Website: theguardian\n",
      "Inter-rater reliability (ICC): 0.95\n"
     ]
    }
   ],
   "source": [
    "import pingouin as pg\n",
    "\n",
    "def flatten_upper_triangle(matrix):\n",
    "    return matrix[np.triu_indices_from(matrix, 1)]\n",
    "\n",
    "def icc(flat_matrices_by_rater):\n",
    "    df = pd.DataFrame(flat_matrices_by_rater)\n",
    "    \n",
    "    df['Target'] = df.index \n",
    "    df_long = pd.melt(df, id_vars='Target', var_name='Rater', value_name='Similarity')\n",
    "    \n",
    "    icc_results = pg.intraclass_corr(data=df_long, targets='Target', raters='Rater', ratings='Similarity')\n",
    "    icc_value = icc_results.iloc[0]['ICC']\n",
    "    \n",
    "    return round(icc_value, 2)\n",
    "\n",
    "def find_icc_per_website(base_dir):\n",
    "    for category in os.listdir(base_dir):\n",
    "        category_path = os.path.join(base_dir, category)\n",
    "        \n",
    "        if not os.path.isdir(category_path):\n",
    "            continue\n",
    "        \n",
    "        files = sorted([f for f in os.listdir(category_path) if f.endswith('.csv')])\n",
    "\n",
    "        matrices_by_rater = {}\n",
    "        \n",
    "        for file in files:\n",
    "            filename, _ = os.path.splitext(file)\n",
    "            rater = filename[-1]\n",
    "            \n",
    "            if rater not in matrices_by_rater:\n",
    "                matrices_by_rater[rater] = []\n",
    "            \n",
    "            matrix = read_matrix_from_csv(os.path.join(category_path, file))\n",
    "            matrices_by_rater[rater].append(matrix)\n",
    "        \n",
    "        flat_matrices_by_rater = {}\n",
    "        \n",
    "        for rater, matrices in matrices_by_rater.items():\n",
    "            flat_matrices = [flatten_upper_triangle(matrix) for matrix in matrices]\n",
    "            flat_data = np.concatenate(flat_matrices)\n",
    "            flat_matrices_by_rater[f'Rater_{rater}'] = flat_data\n",
    "        \n",
    "        if len(flat_matrices_by_rater) > 1:\n",
    "            inter_rater_compatibility = icc(flat_matrices_by_rater)\n",
    "            print(f'Website: {category}')\n",
    "            print(f'Inter-rater reliability (ICC): {inter_rater_compatibility}')\n",
    "        else:\n",
    "            print(f'Not enough matrices to compute ICC for category: {category}')\n",
    "\n",
    "find_icc_per_website('true_labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the similairty matrix for each category in the test folder and saving the results to test.json<br>\n",
    "Feel free to add to / remove from the test folder - **I first generate images using the web scraping module above and then only pick 2-3 categories with 4-5 images each to test because of the cost of api usage**\n",
    "\n",
    "***** Please only run the below cell for *new* images added to the test folder because previous results have already been computed and kept in the test.json file - rerunning for the same images will only cost more without any benefit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = 'test'\n",
    "process_categories(test_dir, static_folder=None) # dynamic few shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = compute_rmse(pred_folder = 'pred_labels/with_fewshot', labels_folder = 'true_labels', tolerance=1)\n",
    "print(f'Normalised rmse (dynamic few shot): {rmse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_categories(test_dir, zero_shot=True) # zero shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalised rmse (zero shot): 0.18257418583505536\n"
     ]
    }
   ],
   "source": [
    "rmse = compute_rmse(pred_folder = 'pred_labels/without_fewshot', labels_folder = 'true_labels', tolerance=1)\n",
    "print(f'Normalised rmse (zero shot): {rmse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Value Add of Images in a Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_links(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    article_links = []\n",
    "\n",
    "    for a_tag in soup.find_all('a', href=True):\n",
    "        if 'article' in a_tag['href']:\n",
    "            heading = a_tag.get('aria-label', None) or a_tag.text.strip()\n",
    "            if heading:  \n",
    "                article_links.append({\n",
    "                    'url': a_tag['href'],\n",
    "                    'heading': heading\n",
    "                })\n",
    "\n",
    "    for heading_tag in soup.find_all(['h1', 'h2', 'h3']):\n",
    "        try:\n",
    "            a_tag = heading_tag.find('a', href=True)\n",
    "            if a_tag in a_tag['href']:\n",
    "                heading = heading_tag.text.strip()\n",
    "                if heading: \n",
    "                    if not any(link['url'] == a_tag['href'] for link in article_links):\n",
    "                        article_links.append({\n",
    "                            'url': a_tag['href'],\n",
    "                            'heading': heading\n",
    "                        })\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return article_links\n",
    "\n",
    "def scrape_article(url, heading):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    article_heading = heading\n",
    "\n",
    "    images = []\n",
    "    for img in soup.find_all('img'):\n",
    "        if img.has_attr('width') and img.has_attr('height'):\n",
    "            width = float(img['width'])\n",
    "            height = float(img['height'])\n",
    "            if width > 150 and height > 150:\n",
    "                images.append({\n",
    "                    'image_url': img['src'],\n",
    "                    'image_alt': img.get('alt', 'No Alt')\n",
    "                })\n",
    "    \n",
    "    main_content = soup.find(id='main-content') or soup.find(id='maincontent') or soup.find(class_='main-content') or soup.find(class_='maincontent')\n",
    "    \n",
    "    content = ''\n",
    "    if main_content:\n",
    "        paragraphs = main_content.find_all('p')\n",
    "        content = ' '.join([p.text for p in paragraphs])\n",
    "\n",
    "    return {\n",
    "        'article_url': url,\n",
    "        'heading': article_heading,\n",
    "        'images': images,\n",
    "        'content': content\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://www.theguardian.com/uk'\n",
    "article_links = get_article_links(base_url)\n",
    "\n",
    "scraped_data = []\n",
    "for article in article_links:\n",
    "    full_url = article['url'] if article['url'].startswith('http') else urljoin(base_url,article[\"url\"])\n",
    "    article_data = scrape_article(full_url, article['heading'])\n",
    "    scraped_data.append(article_data)\n",
    "\n",
    "df = pd.DataFrame(scraped_data)\n",
    "df = df.drop_duplicates(subset='article_url').reset_index(drop=True)\n",
    "\n",
    "base_url_name = urlparse(base_url).netloc.split('.')[1]\n",
    "df.to_csv(f\"article_csvs/{base_url_name}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_url</th>\n",
       "      <th>heading</th>\n",
       "      <th>images</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.theguardian.com/music/article/2024...</td>\n",
       "      <td>‘Harry would approve’: rail firm protects Ches...</td>\n",
       "      <td>[{'image_url': 'https://i.guim.co.uk/img/media...</td>\n",
       "      <td>It has survived two world wars and nearly 200 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.theguardian.com/us-news/article/20...</td>\n",
       "      <td>Kamala Harris’s much-hyped, first big intervie...</td>\n",
       "      <td>[{'image_url': 'https://i.guim.co.uk/img/media...</td>\n",
       "      <td>Donald Trump spent Thursday in Michigan raving...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.theguardian.com/global-development...</td>\n",
       "      <td>‘When life gets hard, you must be harder’: run...</td>\n",
       "      <td>[{'image_url': 'https://i.guim.co.uk/img/media...</td>\n",
       "      <td>The town of Qaim lies on the border of Iraq an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.theguardian.com/society/article/20...</td>\n",
       "      <td>‘I don’t see how it’s enforceable’: pubgoers r...</td>\n",
       "      <td>[{'image_url': 'https://i.guim.co.uk/img/media...</td>\n",
       "      <td>On an average weekend in Moseley, a suburb of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.theguardian.com/music/article/2024...</td>\n",
       "      <td>‘I’m intrigued by failure’: Kim Deal on death,...</td>\n",
       "      <td>[{'image_url': 'https://i.guim.co.uk/img/media...</td>\n",
       "      <td>In all of her decades as one of rock’s great f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         article_url  \\\n",
       "0  https://www.theguardian.com/music/article/2024...   \n",
       "1  https://www.theguardian.com/us-news/article/20...   \n",
       "2  https://www.theguardian.com/global-development...   \n",
       "3  https://www.theguardian.com/society/article/20...   \n",
       "4  https://www.theguardian.com/music/article/2024...   \n",
       "\n",
       "                                             heading  \\\n",
       "0  ‘Harry would approve’: rail firm protects Ches...   \n",
       "1  Kamala Harris’s much-hyped, first big intervie...   \n",
       "2  ‘When life gets hard, you must be harder’: run...   \n",
       "3  ‘I don’t see how it’s enforceable’: pubgoers r...   \n",
       "4  ‘I’m intrigued by failure’: Kim Deal on death,...   \n",
       "\n",
       "                                              images  \\\n",
       "0  [{'image_url': 'https://i.guim.co.uk/img/media...   \n",
       "1  [{'image_url': 'https://i.guim.co.uk/img/media...   \n",
       "2  [{'image_url': 'https://i.guim.co.uk/img/media...   \n",
       "3  [{'image_url': 'https://i.guim.co.uk/img/media...   \n",
       "4  [{'image_url': 'https://i.guim.co.uk/img/media...   \n",
       "\n",
       "                                             content  \n",
       "0  It has survived two world wars and nearly 200 ...  \n",
       "1  Donald Trump spent Thursday in Michigan raving...  \n",
       "2  The town of Qaim lies on the border of Iraq an...  \n",
       "3  On an average weekend in Moseley, a suburb of ...  \n",
       "4  In all of her decades as one of rock’s great f...  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df['images'].apply(lambda x: len(x) > 1)].reset_index(drop=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_url</th>\n",
       "      <th>true_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.theguardian.com/technology/article...</td>\n",
       "      <td>[0, 4, 4, 4, 4, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.theguardian.com/music/article/2024...</td>\n",
       "      <td>[4, 3, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.theguardian.com/us-news/article/20...</td>\n",
       "      <td>[0, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.theguardian.com/global-development...</td>\n",
       "      <td>[3, 2, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.theguardian.com/society/article/20...</td>\n",
       "      <td>[3, 2, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         article_url            true_labels\n",
       "0  https://www.theguardian.com/technology/article...  [0, 4, 4, 4, 4, 0, 0]\n",
       "1  https://www.theguardian.com/music/article/2024...           [4, 3, 0, 0]\n",
       "2  https://www.theguardian.com/us-news/article/20...                 [0, 2]\n",
       "3  https://www.theguardian.com/global-development...              [3, 2, 1]\n",
       "4  https://www.theguardian.com/society/article/20...           [3, 2, 0, 0]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "def convert_to_list(text):\n",
    "    return ast.literal_eval(text)\n",
    "\n",
    "true_labels = pd.read_csv('article_csvs/true_labels.csv', converters={'true_labels': convert_to_list})\n",
    "\n",
    "true_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_url</th>\n",
       "      <th>heading</th>\n",
       "      <th>images</th>\n",
       "      <th>content</th>\n",
       "      <th>true_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.theguardian.com/music/article/2024...</td>\n",
       "      <td>‘Harry would approve’: rail firm protects Ches...</td>\n",
       "      <td>[{'image_url': 'https://i.guim.co.uk/img/media...</td>\n",
       "      <td>It has survived two world wars and nearly 200 ...</td>\n",
       "      <td>[4, 3, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.theguardian.com/us-news/article/20...</td>\n",
       "      <td>Kamala Harris’s much-hyped, first big intervie...</td>\n",
       "      <td>[{'image_url': 'https://i.guim.co.uk/img/media...</td>\n",
       "      <td>Donald Trump spent Thursday in Michigan raving...</td>\n",
       "      <td>[0, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.theguardian.com/global-development...</td>\n",
       "      <td>‘When life gets hard, you must be harder’: run...</td>\n",
       "      <td>[{'image_url': 'https://i.guim.co.uk/img/media...</td>\n",
       "      <td>The town of Qaim lies on the border of Iraq an...</td>\n",
       "      <td>[3, 2, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.theguardian.com/society/article/20...</td>\n",
       "      <td>‘I don’t see how it’s enforceable’: pubgoers r...</td>\n",
       "      <td>[{'image_url': 'https://i.guim.co.uk/img/media...</td>\n",
       "      <td>On an average weekend in Moseley, a suburb of ...</td>\n",
       "      <td>[3, 2, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.theguardian.com/film/article/2024/...</td>\n",
       "      <td>‘You laugh the hardest in grief’: And Mrs, the...</td>\n",
       "      <td>[{'image_url': 'https://i.guim.co.uk/img/media...</td>\n",
       "      <td>Finding the funny side of bereavement may be t...</td>\n",
       "      <td>[4, 2]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         article_url  \\\n",
       "0  https://www.theguardian.com/music/article/2024...   \n",
       "1  https://www.theguardian.com/us-news/article/20...   \n",
       "2  https://www.theguardian.com/global-development...   \n",
       "3  https://www.theguardian.com/society/article/20...   \n",
       "4  https://www.theguardian.com/film/article/2024/...   \n",
       "\n",
       "                                             heading  \\\n",
       "0  ‘Harry would approve’: rail firm protects Ches...   \n",
       "1  Kamala Harris’s much-hyped, first big intervie...   \n",
       "2  ‘When life gets hard, you must be harder’: run...   \n",
       "3  ‘I don’t see how it’s enforceable’: pubgoers r...   \n",
       "4  ‘You laugh the hardest in grief’: And Mrs, the...   \n",
       "\n",
       "                                              images  \\\n",
       "0  [{'image_url': 'https://i.guim.co.uk/img/media...   \n",
       "1  [{'image_url': 'https://i.guim.co.uk/img/media...   \n",
       "2  [{'image_url': 'https://i.guim.co.uk/img/media...   \n",
       "3  [{'image_url': 'https://i.guim.co.uk/img/media...   \n",
       "4  [{'image_url': 'https://i.guim.co.uk/img/media...   \n",
       "\n",
       "                                             content   true_labels  \n",
       "0  It has survived two world wars and nearly 200 ...  [4, 3, 0, 0]  \n",
       "1  Donald Trump spent Thursday in Michigan raving...        [0, 2]  \n",
       "2  The town of Qaim lies on the border of Iraq an...     [3, 2, 1]  \n",
       "3  On an average weekend in Moseley, a suburb of ...  [3, 2, 0, 0]  \n",
       "4  Finding the funny side of bereavement may be t...        [4, 2]  "
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df = df.merge(true_labels, on='article_url', how='inner')\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_url</th>\n",
       "      <th>heading</th>\n",
       "      <th>images</th>\n",
       "      <th>content</th>\n",
       "      <th>true_labels</th>\n",
       "      <th>pred_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.theguardian.com/music/article/2024...</td>\n",
       "      <td>‘Harry would approve’: rail firm protects Ches...</td>\n",
       "      <td>[{'image_url': 'https://i.guim.co.uk/img/media...</td>\n",
       "      <td>It has survived two world wars and nearly 200 ...</td>\n",
       "      <td>[4, 3, 0, 0]</td>\n",
       "      <td>[4, 4, 3, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.theguardian.com/us-news/article/20...</td>\n",
       "      <td>Kamala Harris’s much-hyped, first big intervie...</td>\n",
       "      <td>[{'image_url': 'https://i.guim.co.uk/img/media...</td>\n",
       "      <td>Donald Trump spent Thursday in Michigan raving...</td>\n",
       "      <td>[0, 2]</td>\n",
       "      <td>[0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.theguardian.com/global-development...</td>\n",
       "      <td>‘When life gets hard, you must be harder’: run...</td>\n",
       "      <td>[{'image_url': 'https://i.guim.co.uk/img/media...</td>\n",
       "      <td>The town of Qaim lies on the border of Iraq an...</td>\n",
       "      <td>[3, 2, 1]</td>\n",
       "      <td>[4, 4, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.theguardian.com/society/article/20...</td>\n",
       "      <td>‘I don’t see how it’s enforceable’: pubgoers r...</td>\n",
       "      <td>[{'image_url': 'https://i.guim.co.uk/img/media...</td>\n",
       "      <td>On an average weekend in Moseley, a suburb of ...</td>\n",
       "      <td>[3, 2, 0, 0]</td>\n",
       "      <td>[2, 3, 2, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.theguardian.com/film/article/2024/...</td>\n",
       "      <td>‘You laugh the hardest in grief’: And Mrs, the...</td>\n",
       "      <td>[{'image_url': 'https://i.guim.co.uk/img/media...</td>\n",
       "      <td>Finding the funny side of bereavement may be t...</td>\n",
       "      <td>[4, 2]</td>\n",
       "      <td>[4, 4]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         article_url  \\\n",
       "0  https://www.theguardian.com/music/article/2024...   \n",
       "1  https://www.theguardian.com/us-news/article/20...   \n",
       "2  https://www.theguardian.com/global-development...   \n",
       "3  https://www.theguardian.com/society/article/20...   \n",
       "4  https://www.theguardian.com/film/article/2024/...   \n",
       "\n",
       "                                             heading  \\\n",
       "0  ‘Harry would approve’: rail firm protects Ches...   \n",
       "1  Kamala Harris’s much-hyped, first big intervie...   \n",
       "2  ‘When life gets hard, you must be harder’: run...   \n",
       "3  ‘I don’t see how it’s enforceable’: pubgoers r...   \n",
       "4  ‘You laugh the hardest in grief’: And Mrs, the...   \n",
       "\n",
       "                                              images  \\\n",
       "0  [{'image_url': 'https://i.guim.co.uk/img/media...   \n",
       "1  [{'image_url': 'https://i.guim.co.uk/img/media...   \n",
       "2  [{'image_url': 'https://i.guim.co.uk/img/media...   \n",
       "3  [{'image_url': 'https://i.guim.co.uk/img/media...   \n",
       "4  [{'image_url': 'https://i.guim.co.uk/img/media...   \n",
       "\n",
       "                                             content   true_labels  \\\n",
       "0  It has survived two world wars and nearly 200 ...  [4, 3, 0, 0]   \n",
       "1  Donald Trump spent Thursday in Michigan raving...        [0, 2]   \n",
       "2  The town of Qaim lies on the border of Iraq an...     [3, 2, 1]   \n",
       "3  On an average weekend in Moseley, a suburb of ...  [3, 2, 0, 0]   \n",
       "4  Finding the funny side of bereavement may be t...        [4, 2]   \n",
       "\n",
       "    pred_labels  \n",
       "0  [4, 4, 3, 3]  \n",
       "1        [0, 1]  \n",
       "2     [4, 4, 2]  \n",
       "3  [2, 3, 2, 2]  \n",
       "4        [4, 4]  "
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()\n",
    "api_key = os.getenv('api_key')\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "few_shot_example = merged_df.iloc[5]\n",
    "test_df = merged_df.drop(index=5).reset_index(drop=True)\n",
    "\n",
    "few_shot_prompt = (\n",
    "    f\"Imagine you are an editor evaluating the contribution of images to an article. \"\n",
    "    f\"Your task is to rate each image based on how much it enhances the reader's understanding and engagement with the article. \"\n",
    "    f\"Consider the following criteria for rating each image:\\n\"\n",
    "    f\"- **0: Not Important**: The image does not add any value to the article, is redundant, or is irrelevant.\\n\"\n",
    "    f\"- **1: Slightly Important**: The image adds minimal value, such as illustrating a minor point or providing a generic visual.\\n\"\n",
    "    f\"- **2: Somewhat Important**: The image supports the content but is not crucial for understanding the article.\\n\"\n",
    "    f\"- **3: Moderately Important**: The image is helpful in conveying key ideas or themes, making the article more engaging.\\n\"\n",
    "    f\"- **4: Very Important**: The image is essential, significantly enhancing the article by illustrating a central point, setting context, or evoking strong emotions.\\n\\n\"\n",
    "    f\"Please consider the following factors when rating each image:\\n\"\n",
    "    f\"- **Relevance**: How directly does the image relate to the article's main content?\\n\"\n",
    "    f\"- **Visual Impact**: How much does the image contribute to the reader's emotional engagement or understanding of the article?\\n\"\n",
    "    f\"- **Context**: Is the image contextually appropriate, and does it fit well within the article's narrative?\\n\"\n",
    "    f\"- **Redundancy**: Avoid giving high ratings to images that repeat information or perspectives already provided by previous images.\\n\\n\"\n",
    "    f\"Here is an example to guide your ratings:\\n\\n\"\n",
    "    f\"**Article Heading**: {few_shot_example['heading']}\\n\"\n",
    "    f\"**Article URL**: {few_shot_example['article_url']}\\n\"\n",
    "    f\"**Content**: {few_shot_example['content']}\\n\\n\"\n",
    "    \"### Images and their Alt Texts:\\n\"\n",
    ")\n",
    "\n",
    "for idx, image_info in enumerate(few_shot_example['images']):\n",
    "    few_shot_prompt += (\n",
    "        f\"Image {idx + 1} URL: {image_info['image_url']}\\n\"\n",
    "        f\"Image {idx + 1} Alt Text: {image_info['image_alt']}\\n\"\n",
    "    )\n",
    "\n",
    "few_shot_prompt += f\"\\nTrue Labels: {few_shot_example['true_labels']}\\n\"\n",
    "\n",
    "predicted_labels = []\n",
    "\n",
    "for index, row in test_df.iterrows():\n",
    "    prompt = (\n",
    "        f\"{few_shot_prompt}\\n\\n\"\n",
    "        f\"Now, please rate the images for the following article in the same way (ONLY respond with the ratings - don't add any words):\\n\"\n",
    "        f\"Article Heading: {row['heading']}\\n\"\n",
    "        f\"Article URL: {row['article_url']}\\n\"\n",
    "        f\"Content: {row['content']}\\n\\n\"\n",
    "        \"Images and their Alt Texts:\\n\"\n",
    "    )\n",
    "\n",
    "    for idx, image_info in enumerate(row['images']):\n",
    "        prompt += (\n",
    "            f\"Image {idx + 1} URL: {image_info['image_url']}\\n\"\n",
    "            f\"Image {idx + 1} Alt Text: {image_info['image_alt']}\\n\"\n",
    "        )\n",
    "\n",
    "    prompt += \"\\nPlease provide your ratings:\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an assistant that rates image importance in news articles.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    predicted_rating = response.choices[0].message.content.strip()\n",
    "    cleaned_rating = predicted_rating.replace('[', '').replace(']', '')\n",
    "    predicted_labels.append([int(x.strip()) for x in cleaned_rating.split(',')])\n",
    "\n",
    "\n",
    "test_df.loc[:, 'pred_labels'] = predicted_labels\n",
    "\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rmse(df, tolerance=0):\n",
    "    total_squared_errors = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        true_labels = row['true_labels']\n",
    "        pred_labels = row['pred_labels']\n",
    "        \n",
    "        if len(true_labels) != len(pred_labels):\n",
    "            raise ValueError(f\"Length of true_labels and pred_labels do not match for index {index}.\")\n",
    "\n",
    "        squared_errors = []\n",
    "        for true_label, pred_label in zip(true_labels, pred_labels):\n",
    "            difference = abs(pred_label - true_label)\n",
    "            error = max(0, difference - tolerance)\n",
    "            squared_errors.append(error ** 2)\n",
    "        \n",
    "        total_squared_errors.extend(squared_errors)\n",
    "\n",
    "    mse = np.mean(total_squared_errors)\n",
    "    rmse = np.sqrt(mse)\n",
    "    \n",
    "    return rmse / 4.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.227429413073671"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_rmse(test_df, tolerance=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorization and Web Scraping (Official)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
