{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import base64\n",
    "import time\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web scraping to download images per category in any given news website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_news_categories(url):\n",
    "    # news categories (and associated href) fetched via nav components\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching the URL: {e}\")\n",
    "        return []\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    categories = []\n",
    "    navs = soup.find_all('nav')\n",
    "\n",
    "    for nav in navs:\n",
    "        for link in nav.find_all('a'):\n",
    "            category = link.get_text(strip=True)\n",
    "            category_url = link.get('href')\n",
    "            if category and category_url:\n",
    "                categories.append((category, urljoin(url, category_url)))\n",
    "\n",
    "    return categories\n",
    "\n",
    "def create_directories(base_url, categories):\n",
    "    # create the following dir struct; outputs > base website > categories\n",
    "    base_dir = os.path.join(\"outputs\", urlparse(base_url).netloc)\n",
    "    if not os.path.exists(base_dir):\n",
    "        os.makedirs(base_dir)\n",
    "    \n",
    "    for category, _ in categories:\n",
    "        category_dir = os.path.join(base_dir, category)\n",
    "        if not os.path.exists(category_dir):\n",
    "            os.makedirs(category_dir)\n",
    "\n",
    "    return base_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_image(img_url, save_dir, img_name):\n",
    "    try:\n",
    "        if img_url.startswith('data:'):\n",
    "            save_data_uri_image(img_url, save_dir, img_name)\n",
    "        else:\n",
    "            save_url_image(img_url, save_dir, img_name)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'Error downloading image {img_url}: {e}')\n",
    "\n",
    "def get_extension_from_header(header):\n",
    "    if \"image/jpeg\" in header:\n",
    "        return \".jpg\"\n",
    "    elif \"image/png\" in header:\n",
    "        return \".png\"\n",
    "    elif \"image/xml\" in header:\n",
    "        return \".xml\"\n",
    "    return None\n",
    "\n",
    "def save_data_uri_image(img_url, save_dir, img_name):\n",
    "    header, encoded = img_url.split(',', 1)\n",
    "    data = base64.b64decode(encoded)\n",
    "    ext = get_extension_from_header(header)\n",
    "    if ext:\n",
    "        img_name = img_name.split('.')[0] + ext\n",
    "        img_path = os.path.join(save_dir, img_name)\n",
    "        with open(img_path, 'wb') as f:\n",
    "            f.write(data)\n",
    "\n",
    "def save_url_image(img_url, save_dir, img_name):\n",
    "    response = requests.get(img_url)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    img_path = os.path.join(save_dir, img_name)\n",
    "    with open(img_path, 'wb') as f:\n",
    "        f.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_article(link_tup, base_url, category_url, save_dir, csv_writer, include_content=False):\n",
    "    article_number, link = link_tup\n",
    "    article_url = urljoin(category_url, link.get('href'))\n",
    "    article_response = requests.get(article_url)\n",
    "    article_response.raise_for_status()\n",
    "    article_soup = BeautifulSoup(article_response.content, 'html.parser')\n",
    "    article_heading = extract_heading(link, article_soup)\n",
    "    images = article_soup.find_all('img')\n",
    "    print(len(images))\n",
    "\n",
    "    ############# SPECIFIC FOR WEBSITE #####################\n",
    "\n",
    "    images_to_download = set()\n",
    "    alt_set = set()\n",
    "    for img in images:\n",
    "        if (img.find_parent('picture') or img.find_parent('figure')) \\\n",
    "            and img.find_parent('article', {'tabindex': '-1'}) \\\n",
    "            and img['alt'] not in alt_set:\n",
    "\n",
    "            images_to_download.add(img)\n",
    "            print(img)\n",
    "            alt_set.add(img['alt'])\n",
    "    \n",
    "    content = ''\n",
    "    if include_content:\n",
    "        main_content = article_soup.find('div', class_=\"sp-cn ins_storybody\")\n",
    "    \n",
    "        if main_content:\n",
    "            paragraphs = main_content.find_all('p')\n",
    "            content = ' '.join([p.text for p in paragraphs])\n",
    "    ############# SPECIFIC FOR WEBSITE #####################\n",
    "    \n",
    "    #images_to_download = filter_images(images)\n",
    "    download_and_record_images(article_number, list(images_to_download), base_url, article_url, save_dir, csv_writer, article_heading, content)\n",
    "\n",
    "def extract_heading(link, soup):\n",
    "    heading = link.get('aria-label')\n",
    "    if not heading:\n",
    "        heading = soup.find('h1').get_text(strip=True)\n",
    "    if not heading:\n",
    "        heading = soup.find('h2').get_text(strip=True)\n",
    "    if not heading:\n",
    "        meta_title = soup.find('meta', attrs={'property': 'og:title'}).get_text(strip=True)\n",
    "        if meta_title:\n",
    "            return meta_title.get('content').strip()\n",
    "    if not heading:\n",
    "        heading = soup.find('title').get_text(strip=True)\n",
    "\n",
    "    return heading if heading else 'No Heading'\n",
    "\n",
    "def download_and_record_images(article_number, images_to_download, base_url, article_url, save_dir, csv_writer, article_heading, content=''):\n",
    "    count=0\n",
    "    for img in images_to_download:\n",
    "        src = img.get('src') \n",
    "        img_url = urljoin(article_url, src)\n",
    "\n",
    "        img_name = f'image_{article_number}_{count}.jpg'\n",
    "        count+=1\n",
    "\n",
    "        alt_text = img.get('alt', '')\n",
    "\n",
    "        cols = [base_url, os.path.basename(save_dir), article_number, count, alt_text, article_heading, article_url]\n",
    "        if content:\n",
    "            cols.append(content)\n",
    "\n",
    "        download_image(img_url, save_dir, img_name)\n",
    "        csv_writer.writerow(cols)\n",
    "        \n",
    "        time.sleep(0.001)\n",
    "\n",
    "processed_articles = set()  \n",
    "def download_images(base_url, category_url, save_dir, csv_writer):\n",
    "    response = requests.get(category_url)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    articles = set()\n",
    "\n",
    "    ############# SPECIFIC FOR WEBSITE #####################\n",
    "\n",
    "    for link in soup.find_all('a', href=True):\n",
    "        if 'rcna' in link['href']:\n",
    "            if (link['href'] not in processed_articles):\n",
    "                articles.add(link)\n",
    "                processed_articles.add(link['href'])\n",
    "\n",
    "    ############# SPECIFIC FOR WEBSITE #####################\n",
    "    \n",
    "    article_dict = {i+1: link for i, link in enumerate(list(articles)[:15])}  # Limit to 10\n",
    "    for link_dict in article_dict.items():\n",
    "        process_article(link_dict, base_url, category_url, save_dir, csv_writer)\n",
    "\n",
    "def filter_images(images):\n",
    "    images_to_download = []\n",
    "    for img in images:\n",
    "        width = img.get('width')\n",
    "        if width:\n",
    "            width_is_large = ('%' in width and float(width.replace('%', '')) > 60) or (width.isdigit() and float(width) > 100)\n",
    "    \n",
    "            if width_is_large:\n",
    "                images_to_download.append(img)\n",
    "\n",
    "    return images_to_download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_csv(csv_file_path, include_content=False):\n",
    "    csv_file = open(csv_file_path, mode='w', newline='', encoding='utf-8')\n",
    "    cols = ['website', 'category', 'article_number', 'image number', 'alt', 'article_heading', 'article_url']\n",
    "    if include_content:\n",
    "        cols.append('content')\n",
    "\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "    csv_writer.writerow(cols)\n",
    "    \n",
    "    return csv_file, csv_writer\n",
    "\n",
    "def filter_categories(categories, exclude_keywords):\n",
    "    filtered_categories = []\n",
    "    for category, category_url in categories:\n",
    "        category_words = category.lower().split()\n",
    "        if not any(keyword in category_words for keyword in exclude_keywords):\n",
    "            filtered_categories.append((category, category_url))\n",
    "    return filtered_categories\n",
    "\n",
    "exclude_keywords = ['sign', 'login', 'subscribe', 'advertisement', 'privacy', 'terms', 'contact']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "### jugaru way to get articles - for in article\n",
    "def download_images_2(base_url, category_url, save_dir, csv_writer, include_content=True):\n",
    "    response = requests.get(category_url)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    articles = set()\n",
    "\n",
    "    with open('outputs/file.txt', 'r') as file:\n",
    "        urls = [line.strip() for line in file.readlines()]\n",
    "        \n",
    "    for link in soup.find_all('a', href=True):\n",
    "        href = link['href']\n",
    "        if href.startswith('//'):\n",
    "            href = href.lstrip('//')\n",
    "        print(urljoin(base_url, href))\n",
    "\n",
    "        ### change back to category url\n",
    "        if urljoin(base_url, href) in urls and href not in processed_articles:\n",
    "            articles.add(link)\n",
    "            processed_articles.add(href)\n",
    "    \n",
    "    article_dict = {i+1: link for i, link in enumerate(list(articles)[:15])}  # Limit to 10\n",
    "    print(article_dict)\n",
    "\n",
    "    for link_dict in article_dict.items():\n",
    "        process_article(link_dict, base_url, category_url, save_dir, csv_writer, include_content=include_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for categories\n",
    "# categories = [\n",
    "#     (\"2024_elections\", \"https://www.nbcnews.com/politics/2024-presidential-election\"),\n",
    "#     (\"science_space\", \"https://www.nbcnews.com/science/space\")\n",
    "# ]\n",
    "# \n",
    "# #base_url = \"https://nbcnews.com\"\n",
    "# base_dir = create_directories(base_url, categories)\n",
    "# \n",
    "# csv_file, csv_writer = setup_csv(base_dir)\n",
    "# \n",
    "# for category, category_url in tqdm(categories, desc='Downloading images for each category'):\n",
    "#     category_dir = os.path.join(base_dir, category.replace('/', '_'))\n",
    "#     os.makedirs(category_dir, exist_ok=True)\n",
    "#     download_images(base_url, category_url, category_dir, csv_writer)\n",
    "#     \n",
    "# csv_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for in-article\n",
    "base_url = \"https://news.yahoo.com/\"\n",
    "category_url = \"https://news.yahoo.com/tagged/climate-change/\"\n",
    "\n",
    "category_dir = \"outputs/across_article/news.yahoo.com\"\n",
    "if not os.path.exists(category_dir):\n",
    "    os.makedirs(category_dir)\n",
    "\n",
    "csv_file, csv_writer = setup_csv(f'{category_dir}/image_data.csv', include_content=False)\n",
    "\n",
    "os.makedirs(category_dir, exist_ok=True)\n",
    "download_images_2(base_url, category_url, category_dir, csv_writer, include_content=False)\n",
    "    \n",
    "csv_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
